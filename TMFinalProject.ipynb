{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining Final Project 2019 - 2020\n",
    "\n",
    "## Identifying Authors by Their Writings \n",
    "\n",
    "## Authors: \n",
    "- Inês Diogo (m20190301)\n",
    "- Lara Neves (m20190867) \n",
    "- Susana Paço (m20190821)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** To identify the authors of portuguese texts by training a model with labeled texts from the same authors. \n",
    "\n",
    "\n",
    "\n",
    "**First Visual Analysis**\n",
    "* Metadata was detected on the top of most txt's which can turn the bias up in the models. It was agreed that a good idea was to remove this crucial metadata from the corpora and test if it made a significant difference in the final result:\n",
    "    + The name of the authors was detected in most of the metadata;\n",
    "    + References to the authors' work was also detected.\n",
    "* Texts from different eras of portuguese \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installs - please uncomment those packages that you don't have within your system in order to install them\n",
    "\n",
    "import sys\n",
    "#!{sys.executable} -m pip install -U unidecode\n",
    "#!{sys.executable} -m pip install -U keras\n",
    "#!{sys.executable} -m pip install -U tensorflow\n",
    "#!{sys.executable} -m pip install -U nltk\n",
    "#!{sys.executable} -m pip install git+https://github.com/textpipe/textpipe.git\n",
    "#!{sys.executable} -m pip install -U spacy\n",
    "#!{sys.executable} -m  spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1. Data Pre-Processing](#DPP)\n",
    "    \n",
    "    * [1.1. Renaming txt Files](#rename)\n",
    "\n",
    "    * [1.2. Extracting Data](#extract)\n",
    "    \n",
    "    * [1.3. Clearing MetaData](#ClearMD)\n",
    "    \n",
    "* [2. Creating a Baseline](#Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"DPP\">\n",
    "\n",
    "## 1. Data Preprocessing\n",
    "\n",
    "</a>\n",
    "\n",
    "<a class=\"anchor\" id=\"rename\">\n",
    "\n",
    "### 1.1. Renaming .txt Files\n",
    "\n",
    "</a>\n",
    "\n",
    "We will start by renaming the .txt files so there's no duplicates and we create a standardized form to \n",
    "identify each .txt file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_file_name(author):\n",
    "    i = 0\n",
    "    my_dir_path = \"Data/Corpora/train/\" + author\n",
    "    \n",
    "    for filename in os.listdir(my_dir_path): \n",
    "        \n",
    "        #Define the new and old names with directory path\n",
    "        new_name =str(author) + str(i) + \".txt\"\n",
    "        old_name = my_dir_path + '/' + filename \n",
    "        new_name = my_dir_path + '/' + new_name \n",
    "        \n",
    "        #So it doesn't give out an error when it runs for the second time\n",
    "        # rename all the files \n",
    "        if new_name != old_name: #IT STILL GIVES OUT ERROR\n",
    "            os.rename(old_name, new_name) \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = ['AlmadaNegreiros','CamiloCasteloBranco','EcaDeQueiros','JoseRodriguesSantos','JoseSaramago','LuisaMarquesSilva']\n",
    "authors_sigla = ['AN','CCB','EQ','JRS','JS','LMS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WARNING: Only run the following code if the file names are the original otherwise, if it runs a second time, will give an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for a in range(len(authors)):\n",
    "#    change_file_name(authors[a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"extract\">\n",
    "\n",
    "### 1.2. Extracting Data\n",
    "</a>\n",
    "\n",
    "We will now join together all the texts from the different authors' folders into one single dataframe called _traindf_ while replacing the authors names with their initials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a df for one author the respective .txt files in the corpora\n",
    "def create_df_from_txt(author):\n",
    "    my_dir_path = \"Data/Corpora/train/\" + author\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    for file in Path(my_dir_path).iterdir():\n",
    "        with open(file, \"r\",encoding = 'utf8') as file_open:\n",
    "            results[\"id\"].append(file.name)\n",
    "            results[\"text\"].append(file_open.read())\n",
    "            results[\"author\"] = author\n",
    "            file_open.close()\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join together the dataframes from all the authors\n",
    "def join_df(authors):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for a in range(len(authors)):\n",
    "        df = df.append(create_df_from_txt(authors[a]))\n",
    "    df.reset_index(inplace = True, drop = True)    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>AlmadaNegreiros0.txt</td>\n",
       "      <td>Title: A Scena do Odio\\n\\nAuthor: José de Alma...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AlmadaNegreiros1.txt</td>\n",
       "      <td>Title: O Jardim da Pierrette\\n\\nAuthor: José d...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>AlmadaNegreiros2.txt</td>\n",
       "      <td>\\n\\nTitle: A Invenção do Dia Claro\\n\\nAuthor: ...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>AlmadaNegreiros3.txt</td>\n",
       "      <td>\\nTitle: Litoral\\n       A Amadeo de Souza Car...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>AlmadaNegreiros4.txt</td>\n",
       "      <td>\\n\\n\\nEXPOSIÇÃO\\n\\n+amadeo\\nde souza\\ncardoso+...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>LuisaMarquesSilva4.txt</td>\n",
       "      <td>Título\\ne-medo\\n\\nAutora (inspiradíssima)\\nLuí...</td>\n",
       "      <td>LMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>LuisaMarquesSilva5.txt</td>\n",
       "      <td>LISBOA 2050\\nLisboa, ano de 2050. Um Agosto tã...</td>\n",
       "      <td>LMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>LuisaMarquesSilva6.txt</td>\n",
       "      <td>Título\\nUm passeio pelo inferno\\n\\nAutora\\nLuí...</td>\n",
       "      <td>LMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>LuisaMarquesSilva7.txt</td>\n",
       "      <td>Título\\nRapsódia sem dó (maior)\\n\\nAutora\\nLuí...</td>\n",
       "      <td>LMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>LuisaMarquesSilva8.txt</td>\n",
       "      <td>Título\\nA última história\\n\\nAutora (próximo N...</td>\n",
       "      <td>LMS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                               text  \\\n",
       "0     AlmadaNegreiros0.txt  Title: A Scena do Odio\\n\\nAuthor: José de Alma...   \n",
       "1     AlmadaNegreiros1.txt  Title: O Jardim da Pierrette\\n\\nAuthor: José d...   \n",
       "2     AlmadaNegreiros2.txt  \\n\\nTitle: A Invenção do Dia Claro\\n\\nAuthor: ...   \n",
       "3     AlmadaNegreiros3.txt  \\nTitle: Litoral\\n       A Amadeo de Souza Car...   \n",
       "4     AlmadaNegreiros4.txt  \\n\\n\\nEXPOSIÇÃO\\n\\n+amadeo\\nde souza\\ncardoso+...   \n",
       "..                     ...                                                ...   \n",
       "58  LuisaMarquesSilva4.txt  Título\\ne-medo\\n\\nAutora (inspiradíssima)\\nLuí...   \n",
       "59  LuisaMarquesSilva5.txt  LISBOA 2050\\nLisboa, ano de 2050. Um Agosto tã...   \n",
       "60  LuisaMarquesSilva6.txt  Título\\nUm passeio pelo inferno\\n\\nAutora\\nLuí...   \n",
       "61  LuisaMarquesSilva7.txt  Título\\nRapsódia sem dó (maior)\\n\\nAutora\\nLuí...   \n",
       "62  LuisaMarquesSilva8.txt  Título\\nA última história\\n\\nAutora (próximo N...   \n",
       "\n",
       "   author  \n",
       "0      AN  \n",
       "1      AN  \n",
       "2      AN  \n",
       "3      AN  \n",
       "4      AN  \n",
       "..    ...  \n",
       "58    LMS  \n",
       "59    LMS  \n",
       "60    LMS  \n",
       "61    LMS  \n",
       "62    LMS  \n",
       "\n",
       "[63 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Running all the functions\n",
    "\n",
    "#Creating the training data frame\n",
    "traindf = join_df(authors)\n",
    "\n",
    "#Replacing the name of the authors with labels of their initials\n",
    "for i in range(0,len(authors)):\n",
    "    traindf.author = traindf.author.replace(authors[i],authors_sigla[i])\n",
    "traindf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a df for one author the respective .txt files in the corpora\n",
    "def create_df_from_txttest(numberofwords):\n",
    "    my_dir_path = \"Data/Corpora/test/\"+ str(numberofwords)\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    for file in Path(my_dir_path).iterdir():\n",
    "        with open(file, \"r\",encoding = 'utf8') as file_open:\n",
    "            results[\"id\"].append(file.name)\n",
    "            results[\"text\"].append(file_open.read())\n",
    "            results[\"numberofwords\"] = numberofwords\n",
    "            file_open.close()\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_dftest(numberofwords):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for a in range(len(numberofwords)):\n",
    "        df = df.append(create_df_from_txttest(numberofwords[a]))\n",
    "    df.reset_index(inplace = True, drop = True)    \n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberofwords = [1000, 500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Running all the functions\n",
    "\n",
    "#Creating the training data frame\n",
    "testdf = join_dftest(numberofwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>numberofwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>text1.txt</td>\n",
       "      <td>Depois, pouco a pouco, a tranquilidade regress...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>text2.txt</td>\n",
       "      <td>Justamente como se eu tivesse tido a ideia de ...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>text3.txt</td>\n",
       "      <td>Quase um mês depois, a época de exames aproxim...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text  numberofwords\n",
       "0  text1.txt  Depois, pouco a pouco, a tranquilidade regress...           1000\n",
       "1  text2.txt  Justamente como se eu tivesse tido a ideia de ...           1000\n",
       "2  text3.txt  Quase um mês depois, a época de exames aproxim...           1000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checktable\n",
    "testdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"clearMD\">\n",
    "\n",
    "### 1.2. Clearing MetaData\n",
    "</a>\n",
    "\n",
    "The majority of the .txt files have metadata at the beginning. This is unnecessary and may introduce noise in our model, as such it may be a good idea to remove it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install -U spacy\n",
    "#!{sys.executable} -m  spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pt_core_news_sm\n",
    "import spacy\n",
    "#nlp = pt_core_news_sm.load()\n",
    "spacy_nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#author names to remove them from metadata\n",
    "authors = [\"José de Almada Negreiros\", \"José de ALMADA-NEGREIROS\", \"JOSÉ DE ALMADA-NEGREIROS\", \"Almada Negreiros\", \"Camilo Castelo Branco\", \"CAMILLO CASTELLO BRANCO\", \"Eça de Queirós\", \"Eca de Queiros\", \"José Rodrigues dos Santos\",\"Jose Rodrigues dos Santos\", \"JOSÉ RODRIGUES DOS SANTOS\", \"José Saramago\", \"Jose Saramago\", \"JoSÉ SaRamago\", \"Luísa Marques Silva\", \"Luisa Marques Silva\", \"Luísa Marques da Silva\"]  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eça de Queirós</th>\n",
       "      <th>Camilo Castelo Branco</th>\n",
       "      <th>Almada Negreiros</th>\n",
       "      <th>Saramago</th>\n",
       "      <th>José Rodrigues dos Santos</th>\n",
       "      <th>Luísa Marques Silva</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>O Mistério da Estrada de Sintra</td>\n",
       "      <td>Anátema</td>\n",
       "      <td>O Moinho</td>\n",
       "      <td>Terra do Pecado</td>\n",
       "      <td>Comunicação, Difusão Cultural, 1992; Prefácio</td>\n",
       "      <td>Acabou-se!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>O Crime do Padre Amaro</td>\n",
       "      <td>Os Mistérios de Lisboa</td>\n",
       "      <td>Os Outros</td>\n",
       "      <td>Manual de Pintura e Caligrafia</td>\n",
       "      <td>Crónicas de Guerra I - Da Crimeia a Dachau</td>\n",
       "      <td>Sete Histórias por Acontecer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A Tragédia da Rua das Flores</td>\n",
       "      <td>A Filha do Arcediago</td>\n",
       "      <td>23, 2º Andar</td>\n",
       "      <td>Levantado do Chão</td>\n",
       "      <td>Crónicas de Guerra II - De Saigão a Bagdade</td>\n",
       "      <td>e-Medo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Eça de Queirós   Camilo Castelo Branco Almada Negreiros  \\\n",
       "0  O Mistério da Estrada de Sintra                 Anátema         O Moinho   \n",
       "1           O Crime do Padre Amaro  Os Mistérios de Lisboa        Os Outros   \n",
       "2     A Tragédia da Rua das Flores    A Filha do Arcediago    23, 2º Andar    \n",
       "\n",
       "                         Saramago  \\\n",
       "0                 Terra do Pecado   \n",
       "1  Manual de Pintura e Caligrafia   \n",
       "2               Levantado do Chão   \n",
       "\n",
       "                       José Rodrigues dos Santos           Luísa Marques Silva  \n",
       "0  Comunicação, Difusão Cultural, 1992; Prefácio                    Acabou-se!  \n",
       "1     Crónicas de Guerra I - Da Crimeia a Dachau  Sete Histórias por Acontecer  \n",
       "2    Crónicas de Guerra II - De Saigão a Bagdade                        e-Medo  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#book names to stop words\n",
    "import pandas as pd\n",
    "authorsandbooks = pd.read_excel('Data/AuthorsAndBooks.xlsx')\n",
    "authorsandbooks.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#authors work to arrays\n",
    "\n",
    "# Eça Queirós books\n",
    "Eca = authorsandbooks['Eça de Queirós']\n",
    "Eca = Eca.dropna()\n",
    "Eca = pd.array(Eca)\n",
    "\n",
    "\n",
    "#Camilo Castelo Branco books\n",
    "camilo = authorsandbooks['Camilo Castelo Branco']\n",
    "camilo = camilo.dropna()\n",
    "camilo = pd.array(camilo)\n",
    "\n",
    "\n",
    "# Almada Negreiros books\n",
    "Almada = authorsandbooks['Almada Negreiros']\n",
    "Almada = Almada.dropna()\n",
    "Almada = pd.array(Almada)\n",
    "\n",
    "\n",
    "# Saramago books\n",
    "Saramago = authorsandbooks['Saramago']\n",
    "Saramago = Saramago.dropna()\n",
    "Saramago = pd.array(Saramago)\n",
    "\n",
    "\n",
    "# José Rodrigues dos Santos books\n",
    "JRodriguesSantos = authorsandbooks['José Rodrigues dos Santos']\n",
    "JRodriguesSantos = JRodriguesSantos.dropna()\n",
    "JRodriguesSantos = pd.array(JRodriguesSantos)\n",
    "\n",
    "\n",
    "# Luísa Marques Silva books\n",
    "luisaMarquesSilva = authorsandbooks['Luísa Marques Silva']\n",
    "luisaMarquesSilva = luisaMarquesSilva.dropna()\n",
    "luisaMarquesSilva = pd.array(luisaMarquesSilva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm #progressbar\n",
    "\n",
    "def removemetadata(doc):\n",
    "    processed_corpus = []\n",
    "    for i in tqdm(range(len(doc))):\n",
    "        text = doc['text'].iloc[i,]\n",
    "        for w in authors:\n",
    "            text = re.sub(w,\"\",text)\n",
    "        for x in Eca:\n",
    "            text = re.sub(x,\"\",text)\n",
    "        for t in camilo:\n",
    "            text = re.sub(t,\"\",text)\n",
    "        for s in Almada:\n",
    "            text = re.sub(s,\"\",text)\n",
    "        for y in Saramago:\n",
    "            text = re.sub(y,\"\",text)\n",
    "        for n in JRodriguesSantos:\n",
    "            text = re.sub(n,\"\",text)\n",
    "        for m in luisaMarquesSilva:\n",
    "            text = re.sub(m,\"\",text)\n",
    "        processed_corpus.append(text)\n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a75916eb9b441ebb9b5ebee3e60a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=63), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create a column to test the results of removing crucial metadata text\n",
    "traindf['removeMetadata'] = removemetadata(traindf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>removeMetadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>AlmadaNegreiros0.txt</td>\n",
       "      <td>Title: A Scena do Odio\\n\\nAuthor: José de Alma...</td>\n",
       "      <td>AN</td>\n",
       "      <td>Title: \\n\\nAuthor: \\n\\nRelease Date: September...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AlmadaNegreiros1.txt</td>\n",
       "      <td>Title: O Jardim da Pierrette\\n\\nAuthor: José d...</td>\n",
       "      <td>AN</td>\n",
       "      <td>Title: \\n\\nAuthor: \\n\\nRelease Date: September...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>AlmadaNegreiros2.txt</td>\n",
       "      <td>\\n\\nTitle: A Invenção do Dia Claro\\n\\nAuthor: ...</td>\n",
       "      <td>AN</td>\n",
       "      <td>\\n\\nTitle: \\n\\nAuthor: \\n\\nRelease Date: Septe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>AlmadaNegreiros3.txt</td>\n",
       "      <td>\\nTitle: Litoral\\n       A Amadeo de Souza Car...</td>\n",
       "      <td>AN</td>\n",
       "      <td>\\nTitle: \\n       \\n\\nAuthor: \\n\\nContributor:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>AlmadaNegreiros4.txt</td>\n",
       "      <td>\\n\\n\\nEXPOSIÇÃO\\n\\n+amadeo\\nde souza\\ncardoso+...</td>\n",
       "      <td>AN</td>\n",
       "      <td>\\n\\n\\nEXPOSIÇÃO\\n\\n+amadeo\\nde souza\\ncardoso+...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text  \\\n",
       "0  AlmadaNegreiros0.txt  Title: A Scena do Odio\\n\\nAuthor: José de Alma...   \n",
       "1  AlmadaNegreiros1.txt  Title: O Jardim da Pierrette\\n\\nAuthor: José d...   \n",
       "2  AlmadaNegreiros2.txt  \\n\\nTitle: A Invenção do Dia Claro\\n\\nAuthor: ...   \n",
       "3  AlmadaNegreiros3.txt  \\nTitle: Litoral\\n       A Amadeo de Souza Car...   \n",
       "4  AlmadaNegreiros4.txt  \\n\\n\\nEXPOSIÇÃO\\n\\n+amadeo\\nde souza\\ncardoso+...   \n",
       "\n",
       "  author                                     removeMetadata  \n",
       "0     AN  Title: \\n\\nAuthor: \\n\\nRelease Date: September...  \n",
       "1     AN  Title: \\n\\nAuthor: \\n\\nRelease Date: September...  \n",
       "2     AN  \\n\\nTitle: \\n\\nAuthor: \\n\\nRelease Date: Septe...  \n",
       "3     AN  \\nTitle: \\n       \\n\\nAuthor: \\n\\nContributor:...  \n",
       "4     AN  \\n\\n\\nEXPOSIÇÃO\\n\\n+amadeo\\nde souza\\ncardoso+...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"clearTexts\">\n",
    "\n",
    "### 1.4. Cleaning Texts\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from tqdm import tqdm_notebook as tqdm #progressbar\n",
    "from unidecode import unidecode\n",
    "from nltk.stem import RSLPStemmer\n",
    "#nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary functions for preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercasing(text):       \n",
    "    text = text.lower()     \n",
    "    return text\n",
    "\n",
    "def to_string(text):\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "def lemmatization(word):\n",
    "    lem = WordNetLemmatizer()\n",
    "    word =lem.lemmatize(word)\n",
    "    return word\n",
    "\n",
    "def punctuation(word):\n",
    "    word = re.sub('[\\“\\”\\ \"\\-\\'`~!@#$%^&*()_|+=?;:,.<>\\{\\}\\[\\]\\\\\\/]','', word)\n",
    "    return word\n",
    "\n",
    "def stopwords_nltk(word):\n",
    "    stop_words = set(stopwords.words(\"portuguese\")) \n",
    "    return word in stop_words\n",
    "\n",
    "def stopwords_spacy(word):\n",
    "    return spacy_nlp.vocab[word].is_stop\n",
    "\n",
    "def accents(word):\n",
    "    word = unidecode(word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing 1\n",
    "##### with punctuation and no lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_with_punc(doc, column):\n",
    "    processed_corpus = []\n",
    "    for i in tqdm(range(len(doc))):\n",
    "        text = doc[column].iloc[i,]    \n",
    "    \n",
    "        text = lowercasing(text)\n",
    "        \n",
    "        text = to_string(text)\n",
    "\n",
    "        textfinal = []\n",
    "        for word in text:\n",
    "            if stopwords_nltk(punctuation(word)) or stopwords_spacy(punctuation(word)):\n",
    "                word = re.sub('[^\\“\\”\\ \"\\-\\'`~!@#$%^&*()_|+=?;:,.<>\\{\\}\\[\\]\\\\\\/]','', word)\n",
    "            else:\n",
    "                word\n",
    "            textfinal.append(word)\n",
    "\n",
    "        text = \" \".join(textfinal)\n",
    "    \n",
    "        processed_corpus.append(text)\n",
    "         \n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212e04848d9247cc989638ddf4a916b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=63), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#this process takes some time, please have patience\n",
    "traindf['clean_with_punc'] = preprocessing_with_punc(traindf, 'removeMetadata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing 2 \n",
    "##### no punctuation and no lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_no_punc(doc, column):\n",
    "    processed_corpus = []\n",
    "    for i in tqdm(range(len(doc))):\n",
    "        text = doc[column].iloc[i,]    \n",
    "    \n",
    "        text = lowercasing(text)\n",
    "        \n",
    "        text = to_string(text)\n",
    "        \n",
    "        text = [punctuation(word) for word in text]\n",
    "\n",
    "        text = [word for word in text if (not stopwords_nltk(word)) and (not stopwords_spacy(word))]\n",
    "        \n",
    "        text = [accents(word) for word in text] \n",
    "\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "        processed_corpus.append(text)\n",
    "         \n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf['clean_no_punc'] = preprocessing_no_punc(traindf, 'removeMetadata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing 3 \n",
    "##### no punctuation and with lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_clean(doc, column):\n",
    "    processed_corpus = []\n",
    "    for i in tqdm(range(len(doc))):\n",
    "        text = doc[column].iloc[i,]    \n",
    "    \n",
    "        text = lowercasing(text)\n",
    "        \n",
    "        text = to_string(text)\n",
    "        \n",
    "        text = [punctuation(word) for word in text]\n",
    "\n",
    "        text = [word for word in text if (not stopwords_nltk(word)) and (not stopwords_spacy(word))]\n",
    "        \n",
    "        text = [lemmatization(word) for word in text]\n",
    "        \n",
    "        text = [accents(word) for word in text] \n",
    "\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "        processed_corpus.append(text)\n",
    "         \n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf['clean'] = preprocessing_clean(traindf, 'removeMetadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use Accuracy as an evaluation metric? Depends if the dataset is unbalanced. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.iloc[:,2].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is unbalanced in favour of Camilo Castelo Branco which is why we get a higher accuracy when we use the DummyClassifier with most frequent strategy (0.31) compared to other strategies (stratified 0.22) even though it's just classifying all texts as belonging to CCB. As such accuracy may not be the best metric in this case.\n",
    "\n",
    "Instead we will use the F1 Score as it integrates Precision and Recall simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing the text into chunks of 500 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_doc(doc,column,n):\n",
    "    newdf = pd.DataFrame()\n",
    "    newdf = newdf.reindex(columns = ['id','chunks','author']) \n",
    "    for i in tqdm(range(len(doc))):\n",
    "        text = doc[column].iloc[i,]\n",
    "\n",
    "        text = text.split()\n",
    "\n",
    "        chunks = [' '.join(text[j:j+n]) for j in range(0,len(text),n)]\n",
    "\n",
    "        for c in chunks:\n",
    "            data = []\n",
    "            values = [doc['id'].iloc[i,], c, doc['author'].iloc[i,]]\n",
    "            a_dictionary = dict(zip(newdf.columns.tolist(), values))\n",
    "            data.append(a_dictionary)\n",
    "            newdf = newdf.append(data)\n",
    "            \n",
    "    newdf.index =[j for j in range(len(newdf))]  \n",
    "              \n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf_chunks = split_doc(traindf, 'clean' ,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf_chunks.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation \n",
    "We already have a test set defined however there is value in having an evaluation/development set to determine the model performance as it's being trained and to adjust the parameters.\n",
    "\n",
    "Given the relatively small amount of data we chose to use cross-validation rather than splitting the set into training and evaluation sets.\n",
    "\n",
    "We will do so by applying the k-fold crossvalidation method, the choice of number of folds k will be decided according to:\n",
    "\n",
    "\"... there is a bias-variance trade-off associated with the choice of k in k-fold cross-validation. Typically, given these considerations, one performs k-fold cross-validation using k = 5 or k = 10, as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.\" - Page 184, An Introduction to Statistical Learning, 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "#Creating a k-fold instance where the number of splits is 10\n",
    "kf = KFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using accuracy may have to change\n",
    "def evaluate_model(X,y, model):\n",
    "    return model.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"Baseline\">\n",
    "\n",
    "## 2. Creating a Baseline\n",
    "    \n",
    "</a>\n",
    "\n",
    "Our goal now is to create a naíve baseline to compare our model with. It won't involve any pre-processing of data or removal of extra meta data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st Try\n",
    "Using DummyClassifier as a baseline? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_classifier_score(df,text_column,strategy):\n",
    "\n",
    "    X = df.loc[:,text_column]\n",
    "    y = df.loc[:,'author']\n",
    "    dummy_clf = DummyClassifier(strategy=strategy) #stratified, most_frequent\n",
    "    dummy_clf.fit(X, y)\n",
    "    dummy_clf.predict(X)\n",
    "    return dummy_clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_classifier_score(traindf,'text','most_frequent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a Dummy Classifier inside a crossvalidation k-fold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Placeholder dummy classifier\n",
    "def dummy_classifier_model(X,y,strategy):\n",
    "    dummy_clf = DummyClassifier(strategy=strategy) #stratified, most_frequent\n",
    "    model = dummy_clf.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the dummy classifier\n",
    "def avg_score_DC(method,X,y,strategy, metric):\n",
    "    score_train = []\n",
    "    score_eval = []\n",
    "    for train_index, eval_index in method.split(X):\n",
    "        X_train, X_eval = X.iloc[train_index], X.iloc[eval_index]\n",
    "        y_train, y_eval = y.iloc[train_index], y.iloc[eval_index]\n",
    "        ####Define model:\n",
    "        model = dummy_classifier_model(X_train,y_train,strategy)\n",
    "        ###############\n",
    "        if metric == 'accuracy':\n",
    "            value_train = evaluate_model(X_train, y_train, model)\n",
    "            value_eval = evaluate_model(X_eval,y_eval, model)\n",
    "            \n",
    "        elif metric == 'f1 score':\n",
    "            yhat_train = model.predict(X_train)\n",
    "            yhat_eval = model.predict(X_eval)\n",
    "            value_train = f1_score(y_train, yhat_train, average='weighted')\n",
    "            value_eval = f1_score(y_eval, yhat_eval, average='weighted')\n",
    "        score_train.append(value_train)\n",
    "        score_eval.append(value_eval)\n",
    "        \n",
    "    print('Evaluation measure used:', metric)\n",
    "    print('Train:', np.mean(score_train))\n",
    "    print('Evaluation:', np.mean(score_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_score_DC(kf,traindf['clean'],traindf['author'],'stratified','accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_score_DC(kf,traindf['clean'],traindf['author'],'stratified','f1 score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very bad result with dummy classifier as expected. Creating a bag-of-words is probably a better choice: \n",
    "#### 2nd Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def bag_of_words_model(X,y):\n",
    "    count_vect = CountVectorizer(\n",
    "        max_df=0.8,\n",
    "        max_features=10000, \n",
    "        ngram_range=(1,3)\n",
    "    )\n",
    "    \n",
    "    X_count = count_vect.fit_transform(X)\n",
    "    #using frequencies instead of occurrences in order to normalize the counts for \n",
    "    #documents that are larger compared to smaller ones -term frequency\n",
    "    #As well as downscaling weights for words that occur in many documntenst in the corpus\n",
    "    #and are therefore less informative \n",
    "\n",
    "    X_train_tfidf =  TfidfTransformer().fit_transform(X_count)\n",
    "    #Now that we have the features we can train the classifier. \n",
    "    #Let's use a naive bayes classsifier in the multinomial variant as it's the most well suited for word counts\n",
    "    model = MultinomialNB().fit(X_train_tfidf, y)\n",
    "    return model, X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_score_baseline(method,X,y, metric):\n",
    "    #Using Bag-of-Words\n",
    "    score_train = []\n",
    "    score_eval = []\n",
    "    for train_index, eval_index in method.split(X):\n",
    "        X_train, X_eval = X.iloc[train_index], X.iloc[eval_index]\n",
    "        y_train, y_eval = y.iloc[train_index], y.iloc[eval_index]\n",
    "        \n",
    "        ##Define model:\n",
    "        model = bag_of_words_model(X_train,y_train)[0]\n",
    "        \n",
    "        ##Evaluation of baseline model\n",
    "        #The evaluate_model functions needs to use the respective X_train with the tfid transformer.\n",
    "        if metric == 'accuracy':\n",
    "            value_train = evaluate_model(bag_of_words_model(X_train,y_train)[1], y_train, model)\n",
    "            value_eval = evaluate_model(bag_of_words_model(X_eval,y_eval)[1],y_eval, model)\n",
    "            \n",
    "        elif metric == 'f1 score':\n",
    "            yhat_train = model.predict(bag_of_words_model(X_train,y_train)[1])\n",
    "            yhat_eval = model.predict(bag_of_words_model(X_eval,y_eval)[1])\n",
    "            value_train = f1_score(y_train, yhat_train, average='weighted')\n",
    "            value_eval = f1_score(y_eval, yhat_eval, average='weighted')\n",
    "\n",
    "        score_train.append(value_train)\n",
    "        score_eval.append(value_eval)\n",
    "        \n",
    "    return [np.mean(score_train),np.mean(score_eval)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe to save the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores= pd.DataFrame()\n",
    "df_scores = df_scores.reindex(columns = ['testing','metric','training set', 'evaluation set']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_score_baseline(df, doc,column, metric):\n",
    "    data = []\n",
    "    score_train = avg_score_baseline(kf,doc[column],doc['author'], metric)[0]\n",
    "    score_eval = avg_score_baseline(kf,doc[column],doc['author'], metric)[1]\n",
    "    values = [column, metric, score_train, score_eval]\n",
    "    a_dictionary = dict(zip(df.columns.tolist(), values))\n",
    "    data.append(a_dictionary)\n",
    "    df = df.append(data)\n",
    "            \n",
    "    df.index =[j for j in range(len(df))]  \n",
    "    print('Evaluation measure used:', metric)\n",
    "    print('Train:', score_train)\n",
    "    print('Evaluation:', score_eval)              \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud\n",
    "\n",
    "To see a word cloud - or the most frequent words - written by a given author just type in the function one of the following siglas: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_sigla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corresponding to the following authors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud(author,text_column):\n",
    "    word_cloud = \" \".join(traindf[traindf['author']==author][text_column])\n",
    "\n",
    "\n",
    "    # Create and generate a word cloud image:\n",
    "    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(word_cloud)\n",
    "    # Display the generated image:\n",
    "    plt.figure(figsize=(17,7))\n",
    "    plt.title('Word Cloud from ' + author)\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud('JRS','clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud('CCB','clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_grams(corpus, top_k, n):\n",
    "    \"\"\"\n",
    "    Function that receives a list of documents (corpus) and extracts\n",
    "        the top k most frequent n-grams for that corpus.\n",
    "        \n",
    "    :param corpus: list of texts\n",
    "    :param top_k: int with the number of n-grams that we want to extract\n",
    "    :param n: n gram type to be considered \n",
    "             (if n=1 extracts unigrams, if n=2 extracts bigrams, ...)\n",
    "             \n",
    "    :return: Returns a sorted dataframe in which the first column \n",
    "        contains the extracted ngrams and the second column contains\n",
    "        the respective counts\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer(ngram_range=(n, n), max_features=2000).fit(corpus)\n",
    "    \n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    \n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    \n",
    "    words_freq = []\n",
    "    for word, idx in vec.vocabulary_.items():\n",
    "        words_freq.append((word, sum_words[0, idx]))\n",
    "        \n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    top_df = pd.DataFrame(words_freq[:top_k])\n",
    "    top_df.columns = [\"Ngram\", \"Freq\"]\n",
    "    return top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_df = get_top_n_grams(traindf['clean'], top_k=20, n=1)\n",
    "top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_frequencies(top_df):\n",
    "    \"\"\"\n",
    "    Function that receives a dataframe from the \"get_top_n_grams\" function\n",
    "    and plots the frequencies in a bar plot.\n",
    "    \"\"\"\n",
    "    x_labels = top_df[\"Ngram\"][:30]\n",
    "    y_pos = np.arange(len(x_labels))\n",
    "    values = top_df[\"Freq\"][:30]\n",
    "    plt.bar(y_pos, values, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, x_labels)\n",
    "    plt.ylabel('Frequencies')\n",
    "    plt.title('Words')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "plot_frequencies(top_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Importance of Punctuation\n",
    "\n",
    "We know that José Saramago has an interesting use of punctuation and it can be a factor in identifying texts from him. Does this mean that we shouldn't consider punctuations as stop-words? Let's test it.\n",
    "\n",
    "To do so we will use a dummy classifier on two texts from Saramago and two from JRS. Will we see a difference in the performance of the classifier with or without punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_traindf = traindf[traindf.author=='JS'][1:3].append(traindf[traindf.author=='JRS'][1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run the dummy classifier n_iter times and determine the difference between the two. If the classifier is better with punctuation we should see a _positive difference_ meaning that the classifier that runs on the text with punctuation should have a better accuracy score than the one without. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 50000\n",
    "score_punc_test = []\n",
    "for i in range(0,n_iter):\n",
    "    score_punc_test.append(dummy_classifier_score(punc_traindf,'clean_with_punc','stratified')-dummy_classifier_score(punc_traindf,'clean','stratified'))\n",
    "    i += 1\n",
    "score_punc_test = pd.DataFrame(score_punc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_punc_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a very very small change between each classifiers. Which may indicate that there are no difference in the performance of the classifier of JS with or without punctuation. \n",
    "\n",
    "To confirm this, let's do a one sided hypothesis test, where the null hypothesis is that the mean of the difference between the scores is zero, meaning there is no difference between doing a classifier with or without punctuation for José Saramago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "diff_mean = np.mean(score_punc_test)\n",
    "print('mean', diff_mean)\n",
    "tset, pval = ttest_1samp(score_punc_test, 0)\n",
    "print('p-values', pval)\n",
    "if pval < 0.05:    # alpha value is 0.05 or 5%\n",
    "   print(\"Reject the null hypothesis: there are evidence that the classifier performs differently if there's punctuation\")\n",
    "else:\n",
    "  print(\"Do not reject the null hypothesis: there are no evidence that the classifier is better if we mantain the punctuation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the models with different levels of Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = doc_score_baseline(df_scores, traindf, 'text', 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = doc_score_baseline(df_scores, traindf, 'text', 'f1 score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = doc_score_baseline(df_scores, traindf, 'removeMetadata', 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = doc_score_baseline(df_scores, traindf, 'removeMetadata', 'f1 score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = doc_score_baseline(df_scores, traindf, 'clean_with_punc', 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = doc_score_baseline(df_scores, traindf, 'clean_with_punc', 'f1 score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = doc_score_baseline(df_scores, traindf, 'clean_no_punc', 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = doc_score_baseline(df_scores, traindf, 'clean_no_punc', 'f1 score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = doc_score_baseline(df_scores, traindf, 'clean', 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = doc_score_baseline(df_scores, traindf, 'clean', 'f1 score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots of the Accuracy and F1 score for the different baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(axP, metric):\n",
    "    axP.plot(df_scores['testing'].loc[df_scores['metric']== metric], df_scores['training set'].loc[df_scores['metric']== metric], 'g', label = 'Training set')\n",
    "    axP.plot(df_scores['testing'].loc[df_scores['metric']== metric], df_scores['evaluation set'].loc[df_scores['metric']== metric], 'b', label = 'Evaluation set')\n",
    "    axP.set_title(metric)\n",
    "    axP.set_ylabel('Score')\n",
    "    axP.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "grid=plt.GridSpec(6,6, wspace=0.7)\n",
    "ax1 = fig.add_subplot(grid[0:3,0:3])\n",
    "ax2 = fig.add_subplot(grid[0:3,3:6])\n",
    "\n",
    "plot_metric(ax1, 'accuracy')\n",
    "plot_metric(ax2, 'f1 score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
