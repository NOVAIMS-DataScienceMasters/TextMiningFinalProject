{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining Final Project 2019 - 2020 - LSTM\n",
    "\n",
    "## Identifying Authors by Their Writings \n",
    "\n",
    "## Authors: \n",
    "- Lara Neves (m20190867) \n",
    "- Susana Paço (m20190821)\n",
    "- Inês Diogo (m20190301)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** To identify the authors of portuguese texts by training a model with labeled texts from the same authors. \n",
    "\n",
    "\n",
    "\n",
    "**First Visual Analysis**\n",
    "* Metadata was detected on the top of most txt's which can turn the bias up in the models. It was agreed that a good idea was to remove this crucial metadata from the corpora and test if it made a significant difference in the final result:\n",
    "    + The name of the authors was detected in most of the metadata;\n",
    "    + References to the authors works was also detected.\n",
    "* Texts from different eras of portuguese \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installs - please uncomment those packages that you don't have within your system in order to install them\n",
    "\n",
    "import sys\n",
    "#!{sys.executable} -m pip install -U unidecode\n",
    "#!{sys.executable} -m pip install -U keras\n",
    "#!{sys.executable} -m pip install -U tensorflow\n",
    "#!{sys.executable} -m pip install -U nltk\n",
    "#!{sys.executable} -m pip install git+https://github.com/textpipe/textpipe.git\n",
    "#!{sys.executable} -m pip install -U spacy\n",
    "#!{sys.executable} -m  spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1. Data Pre-Processing](#DPP)\n",
    "    \n",
    "    * [1.1. Renaming txt Files](#rename)\n",
    "\n",
    "    * [1.2. Extracting Data](#extract)\n",
    "    \n",
    "    * [1.3. Clearing MetaData](#ClearMD)\n",
    "    \n",
    "* [2. Creating a Baseline](#Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"DPP\">\n",
    "\n",
    "## 1. Data Preprocessing\n",
    "\n",
    "\n",
    "\n",
    "<a class=\"anchor\" id=\"rename\">\n",
    "\n",
    "### 1.1. Renaming .txt Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the names of the .txt files so there's no duplicates and we create a standardized form to \n",
    "#identify each .txt\n",
    "\n",
    "def change_file_name(author):\n",
    "    i = 0\n",
    "    my_dir_path = \"Data/Corpora/train/\" + author\n",
    "    \n",
    "    for filename in os.listdir(my_dir_path): \n",
    "        \n",
    "        #Define the new and old names with directory path\n",
    "        new_name =str(author) + str(i) + \".txt\"\n",
    "        old_name = my_dir_path + '/' + filename \n",
    "        new_name = my_dir_path + '/' + new_name \n",
    "        \n",
    "        #So it doesn't give out an error when it runs for the second time\n",
    "        # rename all the files \n",
    "        if new_name != old_name: #IT STILL GIVES OUT ERROR\n",
    "            os.rename(old_name, new_name) \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = ['AlmadaNegreiros','CamiloCasteloBranco','EcaDeQueiros','JoseRodriguesSantos','JoseSaramago','LuisaMarquesSilva']\n",
    "authors_sigla = ['AN','CCB','EQ','JRS','JS','LMS']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ONLY RUN ONCE IF THE FILE NAMES ARE THE ORIGINAL otherwise, running a second time, will give an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for a in range(len(authors)):\n",
    "#    change_file_name(authors[a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"extract\">\n",
    "\n",
    "### 1.2. Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a df for one author the respective .txt files in the corpora\n",
    "def create_df_from_txt(author):\n",
    "    my_dir_path = \"Data/Corpora/train/\" + author\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    for file in Path(my_dir_path).iterdir():\n",
    "        with open(file, \"r\",encoding = 'utf8') as file_open:\n",
    "            results[\"id\"].append(file.name)\n",
    "            results[\"text\"].append(file_open.read())\n",
    "            results[\"author\"] = author\n",
    "            file_open.close()\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join together the dataframes from all the authors\n",
    "def join_df(authors):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for a in range(len(authors)):\n",
    "        df = df.append(create_df_from_txt(authors[a]))\n",
    "    df.reset_index(inplace = True, drop = True)    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AlmadaNegreiros3.txt</td>\n",
       "      <td>\\nTitle: Litoral\\n       A Amadeo de Souza Car...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AlmadaNegreiros2.txt</td>\n",
       "      <td>\\n\\nTitle: A Invenção do Dia Claro\\n\\nAuthor: ...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AlmadaNegreiros0.txt</td>\n",
       "      <td>Title: A Scena do Odio\\n\\nAuthor: José de Alma...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AlmadaNegreiros1.txt</td>\n",
       "      <td>Title: O Jardim da Pierrette\\n\\nAuthor: José d...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AlmadaNegreiros5.txt</td>\n",
       "      <td>\\n\\n*JOSÉ DE ALMADA-NEGREIROS*\\n\\n\\n*K4\\n\\no q...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>LuisaMarquesSilva3.txt</td>\n",
       "      <td>CONTROL Z\\nChegou a hora de vos contar. Chegou...</td>\n",
       "      <td>LMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>LuisaMarquesSilva2.txt</td>\n",
       "      <td>O terrível caso do botão assassino\\nLuísa Marq...</td>\n",
       "      <td>LMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>LuisaMarquesSilva0.txt</td>\n",
       "      <td>A BELA HISTÓRIA DE DINIS E BEATRIZ OU REQUIEM ...</td>\n",
       "      <td>LMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>LuisaMarquesSilva1.txt</td>\n",
       "      <td>\\n\\n\\nAcabou-se!\\nLuísa Marques da Silva\\n\\nTí...</td>\n",
       "      <td>LMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>LuisaMarquesSilva8.txt</td>\n",
       "      <td>Título\\nA última história\\n\\nAutora (próximo N...</td>\n",
       "      <td>LMS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                               text  \\\n",
       "0     AlmadaNegreiros3.txt  \\nTitle: Litoral\\n       A Amadeo de Souza Car...   \n",
       "1     AlmadaNegreiros2.txt  \\n\\nTitle: A Invenção do Dia Claro\\n\\nAuthor: ...   \n",
       "2     AlmadaNegreiros0.txt  Title: A Scena do Odio\\n\\nAuthor: José de Alma...   \n",
       "3     AlmadaNegreiros1.txt  Title: O Jardim da Pierrette\\n\\nAuthor: José d...   \n",
       "4     AlmadaNegreiros5.txt  \\n\\n*JOSÉ DE ALMADA-NEGREIROS*\\n\\n\\n*K4\\n\\no q...   \n",
       "..                     ...                                                ...   \n",
       "58  LuisaMarquesSilva3.txt  CONTROL Z\\nChegou a hora de vos contar. Chegou...   \n",
       "59  LuisaMarquesSilva2.txt  O terrível caso do botão assassino\\nLuísa Marq...   \n",
       "60  LuisaMarquesSilva0.txt  A BELA HISTÓRIA DE DINIS E BEATRIZ OU REQUIEM ...   \n",
       "61  LuisaMarquesSilva1.txt  \\n\\n\\nAcabou-se!\\nLuísa Marques da Silva\\n\\nTí...   \n",
       "62  LuisaMarquesSilva8.txt  Título\\nA última história\\n\\nAutora (próximo N...   \n",
       "\n",
       "   author  \n",
       "0      AN  \n",
       "1      AN  \n",
       "2      AN  \n",
       "3      AN  \n",
       "4      AN  \n",
       "..    ...  \n",
       "58    LMS  \n",
       "59    LMS  \n",
       "60    LMS  \n",
       "61    LMS  \n",
       "62    LMS  \n",
       "\n",
       "[63 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Running all the functions\n",
    "\n",
    "#Creating the training data frame\n",
    "traindf = join_df(authors)\n",
    "\n",
    "#Replacing the name of the authors with labels of their initials\n",
    "for i in range(0,len(authors)):\n",
    "    traindf.author = traindf.author.replace(authors[i],authors_sigla[i])\n",
    "traindf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a df for one author the respective .txt files in the corpora\n",
    "def create_df_from_txttest(numberofwords):\n",
    "    my_dir_path = \"Data/Corpora/test/\"+ str(numberofwords)\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    for file in Path(my_dir_path).iterdir():\n",
    "        with open(file, \"r\",encoding = 'utf8') as file_open:\n",
    "            results[\"id\"].append(file.name)\n",
    "            results[\"text\"].append(file_open.read())\n",
    "            results[\"numberofwords\"] = numberofwords\n",
    "            file_open.close()\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_dftest(numberofwords):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for a in range(len(numberofwords)):\n",
    "        df = df.append(create_df_from_txttest(numberofwords[a]))\n",
    "    df.reset_index(inplace = True, drop = True)    \n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberofwords = [1000, 500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Running all the functions\n",
    "\n",
    "#Creating the training data frame\n",
    "testdf = join_dftest(numberofwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>numberofwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text6.txt</td>\n",
       "      <td>\"O Senhor ensina pela pena o que o homem não s...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text5.txt</td>\n",
       "      <td>O cahos de cima a descer, a descer com a morta...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text4.txt</td>\n",
       "      <td>Agora, porém, era sem fervor, arrastadamente, ...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text1.txt</td>\n",
       "      <td>Depois, pouco a pouco, a tranquilidade regress...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text3.txt</td>\n",
       "      <td>Quase um mês depois, a época de exames aproxim...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>text2.txt</td>\n",
       "      <td>Justamente como se eu tivesse tido a ideia de ...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>text6.txt</td>\n",
       "      <td>\"O Senhor ensina pela pena o que o homem não s...</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>text5.txt</td>\n",
       "      <td>O cahos de cima a descer, a descer com a morta...</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>text4.txt</td>\n",
       "      <td>Agora, porém, era sem fervor, arrastadamente, ...</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>text1.txt</td>\n",
       "      <td>Depois, pouco a pouco, a tranquilidade regress...</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>text3.txt</td>\n",
       "      <td>Quase um mês depois, a época de exames aproxim...</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>text2.txt</td>\n",
       "      <td>Justamente como se eu tivesse tido a ideia de ...</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text  \\\n",
       "0   text6.txt  \"O Senhor ensina pela pena o que o homem não s...   \n",
       "1   text5.txt  O cahos de cima a descer, a descer com a morta...   \n",
       "2   text4.txt  Agora, porém, era sem fervor, arrastadamente, ...   \n",
       "3   text1.txt  Depois, pouco a pouco, a tranquilidade regress...   \n",
       "4   text3.txt  Quase um mês depois, a época de exames aproxim...   \n",
       "5   text2.txt  Justamente como se eu tivesse tido a ideia de ...   \n",
       "6   text6.txt  \"O Senhor ensina pela pena o que o homem não s...   \n",
       "7   text5.txt  O cahos de cima a descer, a descer com a morta...   \n",
       "8   text4.txt  Agora, porém, era sem fervor, arrastadamente, ...   \n",
       "9   text1.txt  Depois, pouco a pouco, a tranquilidade regress...   \n",
       "10  text3.txt  Quase um mês depois, a época de exames aproxim...   \n",
       "11  text2.txt  Justamente como se eu tivesse tido a ideia de ...   \n",
       "\n",
       "    numberofwords  \n",
       "0            1000  \n",
       "1            1000  \n",
       "2            1000  \n",
       "3            1000  \n",
       "4            1000  \n",
       "5            1000  \n",
       "6             500  \n",
       "7             500  \n",
       "8             500  \n",
       "9             500  \n",
       "10            500  \n",
       "11            500  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From analysis of the texts, these are the supposed y_test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill a y_test column with possible answers\n",
    "\n",
    "\n",
    "\n",
    "testdf.loc[testdf['id'] == 'text1.txt', 'possibleanswer'] = 'JS'\n",
    "testdf.loc[testdf['id'] == 'text2.txt', 'possibleanswer'] = 'AN'\n",
    "testdf.loc[testdf['id'] == 'text3.txt', 'possibleanswer'] = 'unknown'\n",
    "testdf.loc[testdf['id'] == 'text4.txt', 'possibleanswer'] = 'EQ'\n",
    "testdf.loc[testdf['id'] == 'text5.txt', 'possibleanswer'] = 'CCB'\n",
    "testdf.loc[testdf['id'] == 'text6.txt', 'possibleanswer'] = 'JRS'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>numberofwords</th>\n",
       "      <th>possibleanswer</th>\n",
       "      <th>removeMetadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text6.txt</td>\n",
       "      <td>\"O Senhor ensina pela pena o que o homem não s...</td>\n",
       "      <td>1000</td>\n",
       "      <td>JRS</td>\n",
       "      <td>\"O Senhor ensina pela pena o que o homem não s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text5.txt</td>\n",
       "      <td>O cahos de cima a descer, a descer com a morta...</td>\n",
       "      <td>1000</td>\n",
       "      <td>CCB</td>\n",
       "      <td>O cahos de cima a descer, a descer com a morta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text4.txt</td>\n",
       "      <td>Agora, porém, era sem fervor, arrastadamente, ...</td>\n",
       "      <td>1000</td>\n",
       "      <td>EQ</td>\n",
       "      <td>Agora, porém, era sem fervor, arrastadamente, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text1.txt</td>\n",
       "      <td>Depois, pouco a pouco, a tranquilidade regress...</td>\n",
       "      <td>1000</td>\n",
       "      <td>JS</td>\n",
       "      <td>Depois, pouco a pouco, a tranquilidade regress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text3.txt</td>\n",
       "      <td>Quase um mês depois, a época de exames aproxim...</td>\n",
       "      <td>1000</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Quase um mês depois, a época de exames aproxim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>text2.txt</td>\n",
       "      <td>Justamente como se eu tivesse tido a ideia de ...</td>\n",
       "      <td>1000</td>\n",
       "      <td>AN</td>\n",
       "      <td>Justamente como se eu tivesse tido a ideia de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>text6.txt</td>\n",
       "      <td>\"O Senhor ensina pela pena o que o homem não s...</td>\n",
       "      <td>500</td>\n",
       "      <td>JRS</td>\n",
       "      <td>\"O Senhor ensina pela pena o que o homem não s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>text5.txt</td>\n",
       "      <td>O cahos de cima a descer, a descer com a morta...</td>\n",
       "      <td>500</td>\n",
       "      <td>CCB</td>\n",
       "      <td>O cahos de cima a descer, a descer com a morta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>text4.txt</td>\n",
       "      <td>Agora, porém, era sem fervor, arrastadamente, ...</td>\n",
       "      <td>500</td>\n",
       "      <td>EQ</td>\n",
       "      <td>Agora, porém, era sem fervor, arrastadamente, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>text1.txt</td>\n",
       "      <td>Depois, pouco a pouco, a tranquilidade regress...</td>\n",
       "      <td>500</td>\n",
       "      <td>JS</td>\n",
       "      <td>Depois, pouco a pouco, a tranquilidade regress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>text3.txt</td>\n",
       "      <td>Quase um mês depois, a época de exames aproxim...</td>\n",
       "      <td>500</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Quase um mês depois, a época de exames aproxim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>text2.txt</td>\n",
       "      <td>Justamente como se eu tivesse tido a ideia de ...</td>\n",
       "      <td>500</td>\n",
       "      <td>AN</td>\n",
       "      <td>Justamente como se eu tivesse tido a ideia de ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text  \\\n",
       "0   text6.txt  \"O Senhor ensina pela pena o que o homem não s...   \n",
       "1   text5.txt  O cahos de cima a descer, a descer com a morta...   \n",
       "2   text4.txt  Agora, porém, era sem fervor, arrastadamente, ...   \n",
       "3   text1.txt  Depois, pouco a pouco, a tranquilidade regress...   \n",
       "4   text3.txt  Quase um mês depois, a época de exames aproxim...   \n",
       "5   text2.txt  Justamente como se eu tivesse tido a ideia de ...   \n",
       "6   text6.txt  \"O Senhor ensina pela pena o que o homem não s...   \n",
       "7   text5.txt  O cahos de cima a descer, a descer com a morta...   \n",
       "8   text4.txt  Agora, porém, era sem fervor, arrastadamente, ...   \n",
       "9   text1.txt  Depois, pouco a pouco, a tranquilidade regress...   \n",
       "10  text3.txt  Quase um mês depois, a época de exames aproxim...   \n",
       "11  text2.txt  Justamente como se eu tivesse tido a ideia de ...   \n",
       "\n",
       "    numberofwords possibleanswer  \\\n",
       "0            1000            JRS   \n",
       "1            1000            CCB   \n",
       "2            1000             EQ   \n",
       "3            1000             JS   \n",
       "4            1000        unknown   \n",
       "5            1000             AN   \n",
       "6             500            JRS   \n",
       "7             500            CCB   \n",
       "8             500             EQ   \n",
       "9             500             JS   \n",
       "10            500        unknown   \n",
       "11            500             AN   \n",
       "\n",
       "                                       removeMetadata  \n",
       "0   \"O Senhor ensina pela pena o que o homem não s...  \n",
       "1   O cahos de cima a descer, a descer com a morta...  \n",
       "2   Agora, porém, era sem fervor, arrastadamente, ...  \n",
       "3   Depois, pouco a pouco, a tranquilidade regress...  \n",
       "4   Quase um mês depois, a época de exames aproxim...  \n",
       "5   Justamente como se eu tivesse tido a ideia de ...  \n",
       "6   \"O Senhor ensina pela pena o que o homem não s...  \n",
       "7   O cahos de cima a descer, a descer com a morta...  \n",
       "8   Agora, porém, era sem fervor, arrastadamente, ...  \n",
       "9   Depois, pouco a pouco, a tranquilidade regress...  \n",
       "10  Quase um mês depois, a época de exames aproxim...  \n",
       "11  Justamente como se eu tivesse tido a ideia de ...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"clearMD\">\n",
    "\n",
    "### 1.2. Clearing MetaData\n",
    "</a>\n",
    "\n",
    "The majority of the .txt files have metadata at the beginning. This is unnecessary and may introduce noise in our model, as such it may be a good idea to remove it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#author names to remove them from metadata\n",
    "authors = [\"José de Almada Negreiros\", \"José de ALMADA-NEGREIROS\", \"JOSÉ DE ALMADA-NEGREIROS\", \"Almada Negreiros\", \"Camilo Castelo Branco\", \"CAMILLO CASTELLO BRANCO\", \"Eça de Queirós\", \"Eca de Queiros\", \"José Rodrigues dos Santos\",\"Jose Rodrigues dos Santos\", \"JOSÉ RODRIGUES DOS SANTOS\", \"José Saramago\", \"Jose Saramago\", \"JoSÉ SaRamago\", \"Luísa Marques Silva\", \"Luisa Marques Silva\", \"Luísa Marques da Silva\"]  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eça de Queirós</th>\n",
       "      <th>Camilo Castelo Branco</th>\n",
       "      <th>Almada Negreiros</th>\n",
       "      <th>Saramago</th>\n",
       "      <th>José Rodrigues dos Santos</th>\n",
       "      <th>Luísa Marques Silva</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O Mistério da Estrada de Sintra</td>\n",
       "      <td>Anátema</td>\n",
       "      <td>O Moinho</td>\n",
       "      <td>Terra do Pecado</td>\n",
       "      <td>Comunicação, Difusão Cultural, 1992; Prefácio</td>\n",
       "      <td>Acabou-se!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O Crime do Padre Amaro</td>\n",
       "      <td>Os Mistérios de Lisboa</td>\n",
       "      <td>Os Outros</td>\n",
       "      <td>Manual de Pintura e Caligrafia</td>\n",
       "      <td>Crónicas de Guerra I - Da Crimeia a Dachau</td>\n",
       "      <td>Sete Histórias por Acontecer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Tragédia da Rua das Flores</td>\n",
       "      <td>A Filha do Arcediago</td>\n",
       "      <td>23, 2º Andar</td>\n",
       "      <td>Levantado do Chão</td>\n",
       "      <td>Crónicas de Guerra II - De Saigão a Bagdade</td>\n",
       "      <td>e-Medo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Eça de Queirós   Camilo Castelo Branco Almada Negreiros  \\\n",
       "0  O Mistério da Estrada de Sintra                 Anátema         O Moinho   \n",
       "1           O Crime do Padre Amaro  Os Mistérios de Lisboa        Os Outros   \n",
       "2     A Tragédia da Rua das Flores    A Filha do Arcediago    23, 2º Andar    \n",
       "\n",
       "                         Saramago  \\\n",
       "0                 Terra do Pecado   \n",
       "1  Manual de Pintura e Caligrafia   \n",
       "2               Levantado do Chão   \n",
       "\n",
       "                       José Rodrigues dos Santos           Luísa Marques Silva  \n",
       "0  Comunicação, Difusão Cultural, 1992; Prefácio                    Acabou-se!  \n",
       "1     Crónicas de Guerra I - Da Crimeia a Dachau  Sete Histórias por Acontecer  \n",
       "2    Crónicas de Guerra II - De Saigão a Bagdade                        e-Medo  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#book names to stop words\n",
    "authorsandbooks = pd.read_excel('Data/AuthorsAndBooks.xlsx')\n",
    "authorsandbooks.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#authors work to arrays\n",
    "\n",
    "# Eça Queirós books\n",
    "Eca = authorsandbooks['Eça de Queirós']\n",
    "Eca = Eca.dropna()\n",
    "Eca = pd.array(Eca)\n",
    "\n",
    "\n",
    "#Camilo Castelo Branco books\n",
    "camilo = authorsandbooks['Camilo Castelo Branco']\n",
    "camilo = camilo.dropna()\n",
    "camilo = pd.array(camilo)\n",
    "\n",
    "\n",
    "# Almada Negreiros books\n",
    "Almada = authorsandbooks['Almada Negreiros']\n",
    "Almada = Almada.dropna()\n",
    "Almada = pd.array(Almada)\n",
    "\n",
    "\n",
    "# Saramago books\n",
    "Saramago = authorsandbooks['Saramago']\n",
    "Saramago = Saramago.dropna()\n",
    "Saramago = pd.array(Saramago)\n",
    "\n",
    "\n",
    "# José Rodrigues dos Santos books\n",
    "JRodriguesSantos = authorsandbooks['José Rodrigues dos Santos']\n",
    "JRodriguesSantos = JRodriguesSantos.dropna()\n",
    "JRodriguesSantos = pd.array(JRodriguesSantos)\n",
    "\n",
    "\n",
    "# Luísa Marques Silva books\n",
    "luisaMarquesSilva = authorsandbooks['Luísa Marques Silva']\n",
    "luisaMarquesSilva = luisaMarquesSilva.dropna()\n",
    "luisaMarquesSilva = pd.array(luisaMarquesSilva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm #progressbar\n",
    "\n",
    "def removemetadata(doc):\n",
    "    processed_corpus = []\n",
    "    for i in tqdm(range(len(doc))):\n",
    "        text = doc['text'].iloc[i,]\n",
    "        for w in authors:\n",
    "            text = re.sub(w,\"\",text)\n",
    "        for x in Eca:\n",
    "            text = re.sub(x,\"\",text)\n",
    "        for t in camilo:\n",
    "            text = re.sub(t,\"\",text)\n",
    "        for s in Almada:\n",
    "            text = re.sub(s,\"\",text)\n",
    "        for y in Saramago:\n",
    "            text = re.sub(y,\"\",text)\n",
    "        for n in JRodriguesSantos:\n",
    "            text = re.sub(n,\"\",text)\n",
    "        for m in luisaMarquesSilva:\n",
    "            text = re.sub(m,\"\",text)\n",
    "        processed_corpus.append(text)\n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da073d269c3e4a47974cfdf398d9a9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=63.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612f4a6b4e514aa49c64226a9edff188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create a column to test the results of removing crucial metadata text\n",
    "traindf['removeMetadata'] = removemetadata(traindf)\n",
    "testdf['removeMetadata'] = removemetadata(testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>removeMetadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AlmadaNegreiros3.txt</td>\n",
       "      <td>\\nTitle: Litoral\\n       A Amadeo de Souza Car...</td>\n",
       "      <td>AN</td>\n",
       "      <td>\\nTitle: \\n       \\n\\nAuthor: \\n\\nContributor:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AlmadaNegreiros2.txt</td>\n",
       "      <td>\\n\\nTitle: A Invenção do Dia Claro\\n\\nAuthor: ...</td>\n",
       "      <td>AN</td>\n",
       "      <td>\\n\\nTitle: \\n\\nAuthor: \\n\\nRelease Date: Septe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AlmadaNegreiros0.txt</td>\n",
       "      <td>Title: A Scena do Odio\\n\\nAuthor: José de Alma...</td>\n",
       "      <td>AN</td>\n",
       "      <td>Title: \\n\\nAuthor: \\n\\nRelease Date: September...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AlmadaNegreiros1.txt</td>\n",
       "      <td>Title: O Jardim da Pierrette\\n\\nAuthor: José d...</td>\n",
       "      <td>AN</td>\n",
       "      <td>Title: \\n\\nAuthor: \\n\\nRelease Date: September...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AlmadaNegreiros5.txt</td>\n",
       "      <td>\\n\\n*JOSÉ DE ALMADA-NEGREIROS*\\n\\n\\n*K4\\n\\no q...</td>\n",
       "      <td>AN</td>\n",
       "      <td>\\n\\n**\\n\\n\\n*K4\\n\\no quadrado\\n\\nAZUL*\\n\\nACAB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text  \\\n",
       "0  AlmadaNegreiros3.txt  \\nTitle: Litoral\\n       A Amadeo de Souza Car...   \n",
       "1  AlmadaNegreiros2.txt  \\n\\nTitle: A Invenção do Dia Claro\\n\\nAuthor: ...   \n",
       "2  AlmadaNegreiros0.txt  Title: A Scena do Odio\\n\\nAuthor: José de Alma...   \n",
       "3  AlmadaNegreiros1.txt  Title: O Jardim da Pierrette\\n\\nAuthor: José d...   \n",
       "4  AlmadaNegreiros5.txt  \\n\\n*JOSÉ DE ALMADA-NEGREIROS*\\n\\n\\n*K4\\n\\no q...   \n",
       "\n",
       "  author                                     removeMetadata  \n",
       "0     AN  \\nTitle: \\n       \\n\\nAuthor: \\n\\nContributor:...  \n",
       "1     AN  \\n\\nTitle: \\n\\nAuthor: \\n\\nRelease Date: Septe...  \n",
       "2     AN  Title: \\n\\nAuthor: \\n\\nRelease Date: September...  \n",
       "3     AN  Title: \\n\\nAuthor: \\n\\nRelease Date: September...  \n",
       "4     AN  \\n\\n**\\n\\n\\n*K4\\n\\no quadrado\\n\\nAZUL*\\n\\nACAB...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"clearTexts\">\n",
    "\n",
    "### 1.4. Cleaning Texts\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from tqdm import tqdm_notebook as tqdm #progressbar\n",
    "from unidecode import unidecode\n",
    "from nltk.stem import RSLPStemmer\n",
    "#nltk.download('rslp')\n",
    "\n",
    "#spacy tools\n",
    "import pt_core_news_sm\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('pt_core_news_sm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary functions for preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercasing(text):       \n",
    "    text = text.lower()     \n",
    "    return text\n",
    "\n",
    "def to_string(text):\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "def lemmatization(word):\n",
    "    lem = WordNetLemmatizer()\n",
    "    word =lem.lemmatize(word)\n",
    "    return word\n",
    "\n",
    "def punctuation(word):\n",
    "    word = re.sub('[\\“\\”\\ \"\\-\\'`~!@#$%^&*()_|+=?;:,.<>\\{\\}\\[\\]\\\\\\/]','', word)\n",
    "    return word\n",
    "\n",
    "def stopwords_nltk(word):\n",
    "    stop_words = set(stopwords.words(\"portuguese\")) \n",
    "    return word in stop_words\n",
    "\n",
    "def stopwords_spacy(word):\n",
    "    return spacy_nlp.vocab[word].is_stop\n",
    "\n",
    "def accents(word):\n",
    "    word = unidecode(word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing 1\n",
    "##### with punctuation and no lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_with_punc(doc, column):\n",
    "    processed_corpus = []\n",
    "    for i in tqdm(range(len(doc))):\n",
    "        text = doc[column].iloc[i,]    \n",
    "    \n",
    "        text = lowercasing(text)\n",
    "        \n",
    "        text = to_string(text)\n",
    "\n",
    "        textfinal = []\n",
    "        for word in text:\n",
    "            if stopwords_nltk(punctuation(word)) or stopwords_spacy(punctuation(word)):\n",
    "                word = re.sub('[^\\“\\”\\ \"\\-\\'`~!@#$%^&*()_|+=?;:,.<>\\{\\}\\[\\]\\\\\\/]','', word)\n",
    "            else:\n",
    "                word\n",
    "            textfinal.append(word)\n",
    "\n",
    "        text = \" \".join(textfinal)\n",
    "    \n",
    "        processed_corpus.append(text)\n",
    "         \n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e209cf710a467f95ce7242331d90cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=63.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-bf5c25b5494d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#this process takes some time, please have patience\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraindf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_with_punc'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing_with_punc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraindf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'removeMetadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtestdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_with_punc'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing_with_punc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'removeMetadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-1feae1add5d8>\u001b[0m in \u001b[0;36mpreprocessing_with_punc\u001b[0;34m(doc, column)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtextfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mstopwords_nltk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstopwords_spacy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[^\\“\\”\\ \"\\-\\'`~!@#$%^&*()_|+=?;:,.<>\\{\\}\\[\\]\\\\\\/]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-00524a191f56>\u001b[0m in \u001b[0;36mstopwords_nltk\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstopwords_nltk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"portuguese\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     23\u001b[0m         return [\n\u001b[1;32m     24\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         ]\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/corpus/reader/api.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \"\"\"\n\u001b[1;32m    212\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, encoding)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeekableUnicodeStreamReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#this process takes some time, please have patience\n",
    "traindf['clean_with_punc'] = preprocessing_with_punc(traindf, 'removeMetadata')\n",
    "testdf['clean_with_punc'] = preprocessing_with_punc(testdf, 'removeMetadata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing 2 \n",
    "##### no punctuation and no lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_no_punc(doc, column):\n",
    "    processed_corpus = []\n",
    "    for i in tqdm(range(len(doc))):\n",
    "        text = doc[column].iloc[i,]    \n",
    "    \n",
    "        text = lowercasing(text)\n",
    "        \n",
    "        text = to_string(text)\n",
    "        \n",
    "        text = [punctuation(word) for word in text]\n",
    "\n",
    "        text = [word for word in text if (not stopwords_nltk(word)) and (not stopwords_spacy(word))]\n",
    "        \n",
    "        text = [accents(word) for word in text] \n",
    "\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "        processed_corpus.append(text)\n",
    "         \n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf['clean_no_punc'] = preprocessing_no_punc(traindf, 'removeMetadata')\n",
    "testdf['clean_no_punc'] = preprocessing_no_punc(testdf, 'removeMetadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing the text into chunks of 500 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_doc(doc,column,n):\n",
    "    newdf = pd.DataFrame()\n",
    "    newdf = newdf.reindex(columns = ['id','chunks','author']) \n",
    "    for i in tqdm(range(len(doc))):\n",
    "        text = doc[column].iloc[i,]\n",
    "\n",
    "        text = text.split()\n",
    "\n",
    "        chunks = [' '.join(text[j:j+n]) for j in range(0,len(text),n)]\n",
    "\n",
    "        for c in chunks:\n",
    "            data = []\n",
    "            values = [doc['id'].iloc[i,], c, doc['author'].iloc[i,]]\n",
    "            a_dictionary = dict(zip(newdf.columns.tolist(), values))\n",
    "            data.append(a_dictionary)\n",
    "            newdf = newdf.append(data)\n",
    "            \n",
    "    newdf.index =[j for j in range(len(newdf))]  \n",
    "              \n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf_chunks = split_doc(traindf, 'clean_no_punc' ,500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf_chunks.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import np_utils\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf_chunks.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Tokenize sentences\n",
    "print(\"Tokenizing in progress...\")\n",
    "#1. Train set\n",
    "text_list_train = list(traindf_chunks['chunks'])\n",
    "#text_list_train_lower = [word.lower() for word in text_list_train]\n",
    "tokenized_text_train = [word_tokenize(i) for i in text_list_train]\n",
    "\n",
    "#2. Test set\n",
    "text_list_test = list(testdf['clean_no_punc'])\n",
    "#text_list_test_lower = [word.lower() for word in text_list_test]\n",
    "tokenized_text_test = [word_tokenize(i) for i in text_list_test]\n",
    "\n",
    "#Create vocabulary from train set only\n",
    "list_of_all_words= list(itertools.chain.from_iterable(tokenized_text_test))\n",
    "vocabulary =sorted(list(set(list_of_all_words)))\n",
    "\n",
    "#Remove stopwords (I found out that it makes no difference but you can try on your own)\n",
    "#vocabulary = [word for word in vocabulary if word not in stopwords.words('english')]\n",
    "\n",
    "#--------------------------Pre-processing train and test sets----------------------------------\n",
    "print('Pre-processing Train set...')\n",
    "tokenized_numbers_train = copy.deepcopy(tokenized_text_train)\n",
    "\n",
    "i=-1\n",
    "for list in tokenized_numbers_train:\n",
    "    i=i+1\n",
    "    j=-1\n",
    "    for number in list:\n",
    "        j = j + 1\n",
    "        if tokenized_numbers_train[i][j] in vocabulary:\n",
    "            tokenized_numbers_train[i][j]= vocabulary.index(number)\n",
    "        else:\n",
    "            tokenized_numbers_train[i][j] = 0\n",
    "\n",
    "tokens_train = pd.DataFrame(tokenized_numbers_train, dtype='int32')\n",
    "tokens_train = tokens_train.fillna(0)\n",
    "tokens_train = tokens_train.astype(int)\n",
    "\n",
    "print('Pre-processing Test set...')\n",
    "tokenized_numbers_test = copy.deepcopy(tokenized_text_test)\n",
    "\n",
    "i=-1\n",
    "for list in tokenized_numbers_test:\n",
    "    i=i+1\n",
    "    j=-1\n",
    "    for number in list:\n",
    "        j = j + 1\n",
    "        if tokenized_numbers_test[i][j] in vocabulary:\n",
    "            tokenized_numbers_test[i][j] = vocabulary.index(number)\n",
    "        else:\n",
    "            tokenized_numbers_test[i][j] = 0\n",
    "\n",
    "tokens_test = pd.DataFrame(tokenized_numbers_test, dtype='int32')\n",
    "tokens_test = tokens_test.fillna(0)\n",
    "tokens_test = tokens_test.astype(int)\n",
    "\n",
    "print('Making some more pre-processing to train and test sets...')\n",
    "\n",
    "#Bring both sets to same shape (Choose how many words to use)\n",
    "max_words_in_sentence=30\n",
    "\n",
    "#Shorten or extend Train set to reach selected length\n",
    "if tokens_train.shape[1]>max_words_in_sentence:\n",
    "    tokens_train = tokens_train.drop(tokens_train.columns[[range(max_words_in_sentence,tokens_train.shape[1])]], axis=1)\n",
    "else:\n",
    "    for col in range(tokens_train.shape[1],max_words_in_sentence):\n",
    "        tokens_train[col]=0\n",
    "\n",
    "#Shorten or extend Test set to reach selected length\n",
    "if tokens_test.shape[1] > max_words_in_sentence:\n",
    "    tokens_test = tokens_test.drop(tokens_test.columns[[range(max_words_in_sentence, tokens_test.shape[1])]],\n",
    "                                     axis=1)\n",
    "else:\n",
    "    for col in range(tokens_test.shape[1], max_words_in_sentence):\n",
    "        tokens_test[col] = 0\n",
    "\n",
    "#------------------------------End of Pre-processing----------------------------------------------------\n",
    "\n",
    "#Define train and Test sets\n",
    "train_x = np.array(tokens_train)\n",
    "train_y = np.array(traindf_chunks['author'])\n",
    "\n",
    "test_x = np.array(tokens_test)\n",
    "\n",
    "encoder1 = LabelEncoder()\n",
    "encoder1.fit(train_y)\n",
    "encoded_train_Y = encoder1.transform(train_y)\n",
    "dummy_train_y = np_utils.to_categorical(encoded_train_Y)\n",
    "dummy_train_y.astype(int)\n",
    "\n",
    "l=len(vocabulary)+1\n",
    "inp=train_x.shape[1]\n",
    "\n",
    "#Build an LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(l, 64,input_length=inp))\n",
    "model.add(LSTM(32, dropout=0.4, recurrent_dropout=0.1))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Fit the model\n",
    "model.fit(train_x, dummy_train_y, epochs=15, batch_size=16, verbose=2)\n",
    "\n",
    "\n",
    "# Predict and write to file\n",
    "results = model.predict(test_x)\n",
    "results = pd.DataFrame(results, columns=['AN','CCB','EQ','JRS','JS','LMS'])\n",
    "results['prediction'] = results[['AN','CCB','EQ','JRS','JS','LMS']].idxmax(axis=1)\n",
    "results ['possibleresult'] = y_test\n",
    "results.insert(0, \"id\", id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(train_x, dummy_train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
