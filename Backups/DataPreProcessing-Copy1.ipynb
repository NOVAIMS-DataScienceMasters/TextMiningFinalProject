{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining Final Project 2019 - 2020\n",
    "\n",
    "## Identifying Authors by Their Writings \n",
    "\n",
    "## Authors: \n",
    "- Lara Neves (m20190867) \n",
    "- Susana Paço (m20190821)\n",
    "- Inês Diogo (m20190301)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "#!{sys.executable} -m pip install git+https://github.com/textpipe/textpipe.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1. Data Pre-Processing](#DPP)\n",
    "    \n",
    "    * [1.1. Renaming txt Files](#rename)\n",
    "\n",
    "    * [1.2. Extracting Data](#extract)\n",
    "    \n",
    "    * [1.3. Clearing MetaData](#ClearMD)\n",
    "    \n",
    "* [2. Creating a Baseline](#Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"DPP\">\n",
    "\n",
    "## 1. Data Preprocessing\n",
    "\n",
    "\n",
    "\n",
    "<a class=\"anchor\" id=\"rename\">\n",
    "\n",
    "### 1.1. Renaming .txt Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the names of the .txt files so there's no duplicates\n",
    "\n",
    "def change_file_name(author):\n",
    "    i = 0\n",
    "    my_dir_path = \"Data/Corpora/train/\" + author\n",
    "    \n",
    "    for filename in os.listdir(my_dir_path): \n",
    "        \n",
    "        #Define the new and old names with directory path\n",
    "        new_name =str(author) + str(i) + \".txt\"\n",
    "        old_name = my_dir_path + '/' + filename \n",
    "        new_name = my_dir_path + '/' + new_name \n",
    "        \n",
    "        #So it doesn't give out an error when it runs for the second time\n",
    "        # rename all the files \n",
    "        if new_name != old_name: #IT STILL GIVES OUT ERROR\n",
    "            os.rename(old_name, new_name) \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = ['AlmadaNegreiros','CamiloCasteloBranco','EcaDeQueiros','JoseRodriguesSantos','JoseSaramago','LuisaMarquesSilva']\n",
    "authors_sigla = ['AN','CCB','EQ','JRS','JS','LMS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ONLY RUN ONCE IF THE FILE NAMES ARE THE ORIGINAL otherwise, running a second time, will give an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for a in range(len(authors)):\n",
    "#    change_file_name(authors[a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"extract\">\n",
    "\n",
    "### 1.2. Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a df for one author the respective .txt files in the corpora\n",
    "def create_df_from_txt(author):\n",
    "    my_dir_path = \"Data/Corpora/train/\" + author\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    for file in Path(my_dir_path).iterdir():\n",
    "        with open(file, \"r\",encoding = 'utf8') as file_open:\n",
    "            results[\"id\"].append(file.name)\n",
    "            results[\"text\"].append(file_open.read())\n",
    "            results[\"author\"] = author\n",
    "            file_open.close()\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join together the dataframes from all the authors\n",
    "def join_df(authors):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for a in range(len(authors)):\n",
    "        df = df.append(create_df_from_txt(authors[a]))\n",
    "    df.reset_index(inplace = True, drop = True)    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AlmadaNegreiros3.txt</td>\n",
       "      <td>\\nTitle: Litoral\\n       A Amadeo de Souza Car...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AlmadaNegreiros2.txt</td>\n",
       "      <td>\\n\\nTitle: A Invenção do Dia Claro\\n\\nAuthor: ...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AlmadaNegreiros0.txt</td>\n",
       "      <td>Title: A Scena do Odio\\n\\nAuthor: José de Alma...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AlmadaNegreiros1.txt</td>\n",
       "      <td>Title: O Jardim da Pierrette\\n\\nAuthor: José d...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AlmadaNegreiros5.txt</td>\n",
       "      <td>\\n\\n*JOSÉ DE ALMADA-NEGREIROS*\\n\\n\\n*K4\\n\\no q...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>LuisaMarquesSilva3.txt</td>\n",
       "      <td>CONTROL Z\\nChegou a hora de vos contar. Chegou...</td>\n",
       "      <td>LMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>LuisaMarquesSilva2.txt</td>\n",
       "      <td>O terrível caso do botão assassino\\nLuísa Marq...</td>\n",
       "      <td>LMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>LuisaMarquesSilva0.txt</td>\n",
       "      <td>A BELA HISTÓRIA DE DINIS E BEATRIZ OU REQUIEM ...</td>\n",
       "      <td>LMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>LuisaMarquesSilva1.txt</td>\n",
       "      <td>\\n\\n\\nAcabou-se!\\nLuísa Marques da Silva\\n\\nTí...</td>\n",
       "      <td>LMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>LuisaMarquesSilva8.txt</td>\n",
       "      <td>Título\\nA última história\\n\\nAutora (próximo N...</td>\n",
       "      <td>LMS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                               text  \\\n",
       "0     AlmadaNegreiros3.txt  \\nTitle: Litoral\\n       A Amadeo de Souza Car...   \n",
       "1     AlmadaNegreiros2.txt  \\n\\nTitle: A Invenção do Dia Claro\\n\\nAuthor: ...   \n",
       "2     AlmadaNegreiros0.txt  Title: A Scena do Odio\\n\\nAuthor: José de Alma...   \n",
       "3     AlmadaNegreiros1.txt  Title: O Jardim da Pierrette\\n\\nAuthor: José d...   \n",
       "4     AlmadaNegreiros5.txt  \\n\\n*JOSÉ DE ALMADA-NEGREIROS*\\n\\n\\n*K4\\n\\no q...   \n",
       "..                     ...                                                ...   \n",
       "58  LuisaMarquesSilva3.txt  CONTROL Z\\nChegou a hora de vos contar. Chegou...   \n",
       "59  LuisaMarquesSilva2.txt  O terrível caso do botão assassino\\nLuísa Marq...   \n",
       "60  LuisaMarquesSilva0.txt  A BELA HISTÓRIA DE DINIS E BEATRIZ OU REQUIEM ...   \n",
       "61  LuisaMarquesSilva1.txt  \\n\\n\\nAcabou-se!\\nLuísa Marques da Silva\\n\\nTí...   \n",
       "62  LuisaMarquesSilva8.txt  Título\\nA última história\\n\\nAutora (próximo N...   \n",
       "\n",
       "   author  \n",
       "0      AN  \n",
       "1      AN  \n",
       "2      AN  \n",
       "3      AN  \n",
       "4      AN  \n",
       "..    ...  \n",
       "58    LMS  \n",
       "59    LMS  \n",
       "60    LMS  \n",
       "61    LMS  \n",
       "62    LMS  \n",
       "\n",
       "[63 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Running all the functions\n",
    "\n",
    "#Creating the training data frame\n",
    "traindf = join_df(authors)\n",
    "\n",
    "#Replacing the name of the authors with labels of their initials\n",
    "for i in range(0,len(authors)):\n",
    "    traindf.author = traindf.author.replace(authors[i],authors_sigla[i])\n",
    "traindf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"clearMD\">\n",
    "\n",
    "### 1.2. Clearing MetaData\n",
    "</a>\n",
    "\n",
    "The majority of the .txt files have metadata at the beginning. This is unnecessary and may introduce noise in our model, as such it may be a good idea to remove it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with just one of the .txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AlmadaNegreiros3.txt</td>\n",
       "      <td>\\nTitle: Litoral\\n       A Amadeo de Souza Car...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text  \\\n",
       "0  AlmadaNegreiros3.txt  \\nTitle: Litoral\\n       A Amadeo de Souza Car...   \n",
       "\n",
       "  author  \n",
       "0     AN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = traindf.iloc[0:1]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5f4ff90d9172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"parser\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_custom_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcustom_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "def set_custom_boundaries(doc):\n",
    "    # Adds support to use '\\n\\n\\n' as the delimiter for sentence detection\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == '\\n\\n\\n':\n",
    "            doc[token.i+1].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm',disable=[\"parser\"])\n",
    "nlp.add_pipe(set_custom_boundaries)\n",
    "custom_sentences = nlp(test.iloc[0,1])\n",
    "custom_sentences = list(custom_sentences.sents)\n",
    "\n",
    "i=0\n",
    "for sentence in custom_sentences:\n",
    "    print('**SPACE', i, '**', sentence)\n",
    "    i +=1\n",
    "#nlp_pt = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'custom_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e8d35ae42536>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_nometa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'custom_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "text_nometa = custom_sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WIP.... \n",
    "#Works for a few of the .txt files if the meta data is just at the beginning but not if there are too many large spaces (/n/n/n) or if there are spaces between the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# removing crucial metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install -U spacy\n",
    "#!{sys.executable} -m  spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pt_core_news_sm\n",
    "import spacy\n",
    "#nlp = pt_core_news_sm.load()\n",
    "spacy_nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#author names to remove them from metadata\n",
    "authors = [\"José de Almada Negreiros\", \"José de ALMADA-NEGREIROS\", \"JOSÉ DE ALMADA-NEGREIROS\", \"Almada Negreiros\", \"Camilo Castelo Branco\", \"CAMILLO CASTELLO BRANCO\", \"Eça de Queirós\", \"Eca de Queiros\", \"José Rodrigues dos Santos\",\"Jose Rodrigues dos Santos\", \"JOSÉ RODRIGUES DOS SANTOS\", \"José Saramago\", \"Jose Saramago\", \"JoSÉ SaRamago\", \"Luísa Marques Silva\", \"Luisa Marques Silva\", \"Luísa Marques da Silva\"]  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eça de Queirós</th>\n",
       "      <th>Camilo Castelo Branco</th>\n",
       "      <th>Almada Negreiros</th>\n",
       "      <th>Saramago</th>\n",
       "      <th>José Rodrigues dos Santos</th>\n",
       "      <th>Luísa Marques Silva</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O Mistério da Estrada de Sintra</td>\n",
       "      <td>Anátema</td>\n",
       "      <td>O Moinho</td>\n",
       "      <td>Terra do Pecado</td>\n",
       "      <td>Comunicação, Difusão Cultural, 1992; Prefácio</td>\n",
       "      <td>Acabou-se!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O Crime do Padre Amaro</td>\n",
       "      <td>Os Mistérios de Lisboa</td>\n",
       "      <td>Os Outros</td>\n",
       "      <td>Manual de Pintura e Caligrafia</td>\n",
       "      <td>Crónicas de Guerra I - Da Crimeia a Dachau</td>\n",
       "      <td>Sete Histórias por Acontecer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Tragédia da Rua das Flores</td>\n",
       "      <td>A Filha do Arcediago</td>\n",
       "      <td>23, 2º Andar</td>\n",
       "      <td>Levantado do Chão</td>\n",
       "      <td>Crónicas de Guerra II - De Saigão a Bagdade</td>\n",
       "      <td>e-Medo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Eça de Queirós   Camilo Castelo Branco Almada Negreiros  \\\n",
       "0  O Mistério da Estrada de Sintra                 Anátema         O Moinho   \n",
       "1           O Crime do Padre Amaro  Os Mistérios de Lisboa        Os Outros   \n",
       "2     A Tragédia da Rua das Flores    A Filha do Arcediago    23, 2º Andar    \n",
       "\n",
       "                         Saramago  \\\n",
       "0                 Terra do Pecado   \n",
       "1  Manual de Pintura e Caligrafia   \n",
       "2               Levantado do Chão   \n",
       "\n",
       "                       José Rodrigues dos Santos           Luísa Marques Silva  \n",
       "0  Comunicação, Difusão Cultural, 1992; Prefácio                    Acabou-se!  \n",
       "1     Crónicas de Guerra I - Da Crimeia a Dachau  Sete Histórias por Acontecer  \n",
       "2    Crónicas de Guerra II - De Saigão a Bagdade                        e-Medo  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#book names to stop words\n",
    "import pandas as pd\n",
    "authorsandbooks = pd.read_excel('Data/AuthorsAndBooks.xlsx')\n",
    "authorsandbooks.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#authors work to arrays\n",
    "\n",
    "# Eça Queirós books\n",
    "Eca = authorsandbooks['Eça de Queirós']\n",
    "Eca = Eca.dropna()\n",
    "Eca = pd.array(Eca)\n",
    "\n",
    "\n",
    "#Camilo Castelo Branco books\n",
    "camilo = authorsandbooks['Camilo Castelo Branco']\n",
    "camilo = camilo.dropna()\n",
    "camilo = pd.array(camilo)\n",
    "\n",
    "\n",
    "# Almada Negreiros books\n",
    "Almada = authorsandbooks['Almada Negreiros']\n",
    "Almada = Almada.dropna()\n",
    "Almada = pd.array(Almada)\n",
    "\n",
    "\n",
    "# Saramago books\n",
    "Saramago = authorsandbooks['Saramago']\n",
    "Saramago = Saramago.dropna()\n",
    "Saramago = pd.array(Saramago)\n",
    "\n",
    "\n",
    "# José Rodrigues dos Santos books\n",
    "JRodriguesSantos = authorsandbooks['José Rodrigues dos Santos']\n",
    "JRodriguesSantos = JRodriguesSantos.dropna()\n",
    "JRodriguesSantos = pd.array(JRodriguesSantos)\n",
    "\n",
    "\n",
    "# Luísa Marques Silva books\n",
    "luisaMarquesSilva = authorsandbooks['Luísa Marques Silva']\n",
    "luisaMarquesSilva = luisaMarquesSilva.dropna()\n",
    "luisaMarquesSilva = pd.array(luisaMarquesSilva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm #progressbar\n",
    "\n",
    "def removemetadata(doc):\n",
    "    processed_corpus = []\n",
    "    for i in tqdm(range(len(doc))):\n",
    "        text = doc['text'].iloc[i,]\n",
    "        for w in authors:\n",
    "            text = re.sub(w,\"\",text)\n",
    "        for x in Eca:\n",
    "            text = re.sub(x,\"\",text)\n",
    "        for t in camilo:\n",
    "            text = re.sub(t,\"\",text)\n",
    "        for s in Almada:\n",
    "            text = re.sub(s,\"\",text)\n",
    "        for y in Saramago:\n",
    "            text = re.sub(y,\"\",text)\n",
    "        for n in JRodriguesSantos:\n",
    "            text = re.sub(n,\"\",text)\n",
    "        for m in luisaMarquesSilva:\n",
    "            text = re.sub(m,\"\",text)\n",
    "        processed_corpus.append(text)\n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e66e6046ff46dca6e707e6be9e926a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=63.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create a column to test the results of removing crucial metadata text\n",
    "traindf['removeMetadata'] = removemetadata(traindf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>removeMetadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AlmadaNegreiros3.txt</td>\n",
       "      <td>\\nTitle: Litoral\\n       A Amadeo de Souza Car...</td>\n",
       "      <td>AN</td>\n",
       "      <td>\\nTitle: \\n       \\n\\nAuthor: \\n\\nContributor:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AlmadaNegreiros2.txt</td>\n",
       "      <td>\\n\\nTitle: A Invenção do Dia Claro\\n\\nAuthor: ...</td>\n",
       "      <td>AN</td>\n",
       "      <td>\\n\\nTitle: \\n\\nAuthor: \\n\\nRelease Date: Septe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AlmadaNegreiros0.txt</td>\n",
       "      <td>Title: A Scena do Odio\\n\\nAuthor: José de Alma...</td>\n",
       "      <td>AN</td>\n",
       "      <td>Title: \\n\\nAuthor: \\n\\nRelease Date: September...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AlmadaNegreiros1.txt</td>\n",
       "      <td>Title: O Jardim da Pierrette\\n\\nAuthor: José d...</td>\n",
       "      <td>AN</td>\n",
       "      <td>Title: \\n\\nAuthor: \\n\\nRelease Date: September...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AlmadaNegreiros5.txt</td>\n",
       "      <td>\\n\\n*JOSÉ DE ALMADA-NEGREIROS*\\n\\n\\n*K4\\n\\no q...</td>\n",
       "      <td>AN</td>\n",
       "      <td>\\n\\n**\\n\\n\\n*K4\\n\\no quadrado\\n\\nAZUL*\\n\\nACAB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text  \\\n",
       "0  AlmadaNegreiros3.txt  \\nTitle: Litoral\\n       A Amadeo de Souza Car...   \n",
       "1  AlmadaNegreiros2.txt  \\n\\nTitle: A Invenção do Dia Claro\\n\\nAuthor: ...   \n",
       "2  AlmadaNegreiros0.txt  Title: A Scena do Odio\\n\\nAuthor: José de Alma...   \n",
       "3  AlmadaNegreiros1.txt  Title: O Jardim da Pierrette\\n\\nAuthor: José d...   \n",
       "4  AlmadaNegreiros5.txt  \\n\\n*JOSÉ DE ALMADA-NEGREIROS*\\n\\n\\n*K4\\n\\no q...   \n",
       "\n",
       "  author                                     removeMetadata  \n",
       "0     AN  \\nTitle: \\n       \\n\\nAuthor: \\n\\nContributor:...  \n",
       "1     AN  \\n\\nTitle: \\n\\nAuthor: \\n\\nRelease Date: Septe...  \n",
       "2     AN  Title: \\n\\nAuthor: \\n\\nRelease Date: September...  \n",
       "3     AN  Title: \\n\\nAuthor: \\n\\nRelease Date: September...  \n",
       "4     AN  \\n\\n**\\n\\n\\n*K4\\n\\no quadrado\\n\\nAZUL*\\n\\nACAB...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = spacy_nlp(test.at[0,'text'])\n",
    "tokens = [token.text for token in doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original Article: %s' % (test.at[0,'text']))\n",
    "print()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"clearTexts\">\n",
    "\n",
    "### 1.4. Cleaning Texts\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from tqdm import tqdm_notebook as tqdm #progressbar\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary functions for preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercasing(text):       \n",
    "    text = text.lower()     \n",
    "    return text\n",
    "\n",
    "def to_string(text):\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "def lemmatization(word):\n",
    "    lem = WordNetLemmatizer()\n",
    "    text = lem.lemmatize(word)\n",
    "    return text\n",
    "\n",
    "def punctuation(word):\n",
    "    text = re.sub('[\\“\\”\\ \"\\-\\'`~!@#$%^&*()_|+=?;:,.<>\\{\\}\\[\\]\\\\\\/]','', word)\n",
    "    return text\n",
    "\n",
    "def stopwords_nltk(word):\n",
    "    stop_words = set(stopwords.words(\"portuguese\")) \n",
    "    return word in stop_words\n",
    "\n",
    "def stopwords_spacy(word):\n",
    "    return spacy_nlp.vocab[word].is_stop\n",
    "\n",
    "def accents(word):\n",
    "    word = unidecode(word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing 1 - leaving the punctuation untouched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### First, a function to preprocess a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_punc_text(text):\n",
    "    text = lowercasing(text)\n",
    "    text = to_string(text)\n",
    "\n",
    "    textfinal = []\n",
    "    for word in text:\n",
    "        if stopwords_nltk(punctuation(word)) or stopwords_spacy(punctuation(word)):\n",
    "            word = re.sub('[^\\“\\”\\ \"\\-\\'`~!@#$%^&*()_|+=?;:,.<>\\{\\}\\[\\]\\\\\\/]','', word)\n",
    "        else:\n",
    "            word\n",
    "\n",
    "        word = accents(word)\n",
    "        \n",
    "        word = lemmatization(word) \n",
    "\n",
    "        textfinal.append(word)\n",
    "\n",
    "\n",
    "    text = \" \".join(textfinal)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now, preprocess the doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_punc_doc(doc, column):\n",
    "    processed_corpus = []\n",
    "    for i in tqdm(range(len(doc))):\n",
    "        text = doc[column].iloc[i,]\n",
    "        \n",
    "        text = preprocessing_punc_text(text)\n",
    "        \n",
    "        processed_corpus.append(text)\n",
    "         \n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traindf['clean_with_punc'] = preprocessing_punc_doc(traindf, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing 2 - with the removal of the punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We have to remove again the stop-words because of words like 'foi-se', that without punctuation become two words where one of them is a stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_doc(doc, column):\n",
    "    processed_corpus = []\n",
    "    for i in tqdm(range(len(doc))):\n",
    "        text = doc[column].iloc[i,]\n",
    "        \n",
    "        text = preprocessing_punc_text(text)\n",
    "        \n",
    "        text = re.sub('[^A-Za-z0-9]',' ', text)\n",
    "        \n",
    "        text = to_string(text)\n",
    "        \n",
    "        text = [word for word in text if (not stopwords_nltk(word)) and (not stopwords_spacy(word))]\n",
    "        \n",
    "        text = \" \".join(text)\n",
    "        \n",
    "        processed_corpus.append(text)\n",
    "         \n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traindf['clean'] = preprocessing_doc(traindf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use Accuracy as an evaluation metric? Depends if the dataset is unbalanced. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.iloc[:,2].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is unbalanced in favour of Camilo Castelo Branco which is why we get a higher accuracy when we use the DummyClassifier with most frequent strategy (0.31) compared to other strategies (stratified 0.22) even though it's just classifying all texts as belonging to CCB. As such accuracy may not be the best metric in this case.\n",
    "\n",
    "Instead we will use the F1 Score as it integrates Precision and Recall simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud\n",
    "\n",
    "To see a word cloud - or the most frequent words - written by a given author just type in the function one of the following siglas: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_sigla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corresponding to the following authors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud(author,text_column):\n",
    "    word_cloud = \" \".join(traindf[traindf['author']==author][text_column])\n",
    "\n",
    "\n",
    "    # Create and generate a word cloud image:\n",
    "    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(word_cloud)\n",
    "    # Display the generated image:\n",
    "    plt.figure(figsize=(17,7))\n",
    "    plt.title('Word Cloud from ' + author)\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud('JS','clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WILL NEED TO REMOVE STOP WORDS FIRST BEFORE IT MAKES ANY SENSE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Importance of Punctuation\n",
    "\n",
    "We know that José Saramago has an interesting use of punctuation and it can be a factor in identifying texts from him. Does this mean that we shouldn't consider punctuations as stop-words? Let's test it.\n",
    "\n",
    "To do so we will use a dummy classifier on two texts from Saramago and two from JRS. Will we see a difference in the performance of the classifier with or without punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_traindf = traindf[traindf.author=='JS'][1:3].append(traindf[traindf.author=='JRS'][1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run the dummy classifier n_iter times and determine the difference between the two. If the classifier is better with punctuation we should see a _positive difference_ meaning that the classifier that runs on the text with punctuation should have a better accuracy score than the one without. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 50000\n",
    "score_punc_test = []\n",
    "for i in range(0,n_iter):\n",
    "    score_punc_test.append(dummy_classifier_score(punc_traindf,'clean_with_punc','stratified')-dummy_classifier_score(punc_traindf,'clean','stratified'))\n",
    "    i += 1\n",
    "score_punc_test = pd.DataFrame(score_punc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_punc_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a very very small change between each classifiers. Which may indicate that there are no difference in the performance of the classifier of JS with or without punctuation. \n",
    "\n",
    "To confirm this, let's do a one sided hypothesis test, where the null hypothesis is that the mean of the difference between the scores is zero, meaning there is no difference between doing a classifier with or without punctuation for José Saramago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "diff_mean = np.mean(score_punc_test)\n",
    "print('mean', diff_mean)\n",
    "tset, pval = ttest_1samp(score_punc_test, 0)\n",
    "print('p-values', pval)\n",
    "if pval < 0.05:    # alpha value is 0.05 or 5%\n",
    "   print(\"Reject the null hypothesis: there are evidence that the classifier performs differently if there's punctuation\")\n",
    "else:\n",
    "  print(\"Do not reject the null hypothesis: there are no evidence that the classifier is better if we mantain the punctuation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing the text into chunks of 500 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_doc(doc,column,n):\n",
    "    newdf = pd.DataFrame()\n",
    "    newdf = newdf.reindex(columns = ['id','chunks','author']) \n",
    "    for i in tqdm(range(len(doc))):\n",
    "        text = doc[column].iloc[i,]\n",
    "\n",
    "        text = text.split()\n",
    "\n",
    "        chunks = [' '.join(text[j:j+n]) for j in range(0,len(text),n)]\n",
    "\n",
    "        for c in chunks:\n",
    "            data = []\n",
    "            values = [doc['id'].iloc[i,], c, doc['author'].iloc[i,]]\n",
    "            a_dictionary = dict(zip(newdf.columns.tolist(), values))\n",
    "            data.append(a_dictionary)\n",
    "            newdf = newdf.append(data)\n",
    "            \n",
    "    newdf.index =[j for j in range(len(newdf))]  \n",
    "              \n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunks = split_doc(traindf, 'clean' ,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunks.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation \n",
    "We already have a test set defined however there is value in having an evaluation/development set to determine the model performance as it's being trained and to adjust the parameters.\n",
    "\n",
    "Given the relatively small amount of data we chose to use cross-validation rather than splitting the set into training and evaluation sets.\n",
    "\n",
    "We will do so by applying the k-fold crossvalidation method, the choice of number of folds k will be decided according to:\n",
    "\n",
    "\"... there is a bias-variance trade-off associated with the choice of k in k-fold cross-validation. Typically, given these considerations, one performs k-fold cross-validation using k = 5 or k = 10, as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.\" - Page 184, An Introduction to Statistical Learning, 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "#Creating a k-fold instance where the number of splits is 10\n",
    "kf = KFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using accuracy may have to change\n",
    "def evaluate_model(X,y, model):\n",
    "    return model.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"Baseline\">\n",
    "\n",
    "## 2. Creating a Baseline\n",
    "    \n",
    "</a>\n",
    "\n",
    "Our goal now is to create a naíve baseline to compare our model with. It won't involve any pre-processing of data or removal of extra meta data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st Try\n",
    "Using DummyClassifier as a baseline? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_classifier_score(df,text_column,strategy):\n",
    "\n",
    "    X = df.loc[:,text_column]\n",
    "    y = df.loc[:,'author']\n",
    "    dummy_clf = DummyClassifier(strategy=strategy) #stratified, most_frequent\n",
    "    dummy_clf.fit(X, y)\n",
    "    dummy_clf.predict(X)\n",
    "    return dummy_clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_classifier_score(traindf,'text','most_frequent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a Dummy Classifier inside a crossvalidation k-fold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Placeholder dummy classifier\n",
    "def dummy_classifier_model(X,y,strategy):\n",
    "    dummy_clf = DummyClassifier(strategy=strategy) #stratified, most_frequent\n",
    "    model = dummy_clf.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the dummy classifier\n",
    "def avg_score_DC(method,X,y,strategy):\n",
    "    score_train = []\n",
    "    score_eval = []\n",
    "    for train_index, eval_index in method.split(X):\n",
    "        X_train, X_eval = X.iloc[train_index], X.iloc[eval_index]\n",
    "        y_train, y_eval = y.iloc[train_index], y.iloc[eval_index]\n",
    "        ####Define model:\n",
    "        model = dummy_classifier_model(X_train,y_train,strategy)\n",
    "        ###############\n",
    "        value_train = evaluate_model(X_train, y_train, model)\n",
    "        value_eval = evaluate_model(X_eval,y_eval, model)\n",
    "        score_train.append(value_train)\n",
    "        score_eval.append(value_eval)\n",
    "\n",
    "    print('Train:', np.mean(score_train))\n",
    "    print('Evaluation:', np.mean(score_eval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_score_DC(kf,traindf['clean'],traindf['author'],'stratified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very bad result with dummy classifier as expected. Creating a bag-of-words is probably a better choice: \n",
    "#### 2nd Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def bag_of_words_model(X,y):\n",
    "    count_vect = CountVectorizer(\n",
    "        max_df=0.8,\n",
    "        max_features=10000, \n",
    "        ngram_range=(1,3)\n",
    "    )\n",
    "    \n",
    "    X_count = count_vect.fit_transform(X)\n",
    "    #using frequencies instead of occurrences in order to normalize the counts for \n",
    "    #documents that are larger compared to smaller ones -term frequency\n",
    "    #As well as downscaling weights for words that occur in many documntenst in the corpus\n",
    "    #and are therefore less informative \n",
    "\n",
    "    X_train_tfidf =  TfidfTransformer().fit_transform(X_count)\n",
    "    #Now that we have the features we can train the classifier. \n",
    "    #Let's use a naive bayes classsifier in the multinomial variant as it's the most well suited for word counts\n",
    "    model = MultinomialNB().fit(X_train_tfidf, y)\n",
    "    return model, X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_score_baseline(method,X,y):\n",
    "    #Using Bag-of-Words\n",
    "    score_train = []\n",
    "    score_eval = []\n",
    "    for train_index, eval_index in method.split(X):\n",
    "        X_train, X_eval = X.iloc[train_index], X.iloc[eval_index]\n",
    "        y_train, y_eval = y.iloc[train_index], y.iloc[eval_index]\n",
    "        \n",
    "        ##Define model:\n",
    "        model = bag_of_words_model(X_train,y_train)[0]\n",
    "        \n",
    "        ##Evaluation of baseline model\n",
    "        #The evaluate_model functions needs to use the respective X_train with the tfid transformer.\n",
    "        value_train = evaluate_model(bag_of_words_model(X_train,y_train)[1], y_train, model)\n",
    "        value_eval = evaluate_model(bag_of_words_model(X_eval,y_eval)[1],y_eval, model)\n",
    "        \n",
    "        score_train.append(value_train)\n",
    "        score_eval.append(value_eval)\n",
    "        \n",
    "        ##Average of Evaluation measure\n",
    "    print('Train:', np.mean(score_train))\n",
    "    print('Evaluation:', np.mean(score_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_score_baseline(kf,traindf['clean'],traindf['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_score_baseline(kf,traindf['clean_with_punc'],traindf['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_score_baseline(kf,traindf['text'],traindf['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_grams(corpus, top_k, n):\n",
    "    \"\"\"\n",
    "    Function that receives a list of documents (corpus) and extracts\n",
    "        the top k most frequent n-grams for that corpus.\n",
    "        \n",
    "    :param corpus: list of texts\n",
    "    :param top_k: int with the number of n-grams that we want to extract\n",
    "    :param n: n gram type to be considered \n",
    "             (if n=1 extracts unigrams, if n=2 extracts bigrams, ...)\n",
    "             \n",
    "    :return: Returns a sorted dataframe in which the first column \n",
    "        contains the extracted ngrams and the second column contains\n",
    "        the respective counts\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer(ngram_range=(n, n), max_features=2000).fit(corpus)\n",
    "    \n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    \n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    \n",
    "    words_freq = []\n",
    "    for word, idx in vec.vocabulary_.items():\n",
    "        words_freq.append((word, sum_words[0, idx]))\n",
    "        \n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    top_df = pd.DataFrame(words_freq[:top_k])\n",
    "    top_df.columns = [\"Ngram\", \"Freq\"]\n",
    "    return top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_df = get_top_n_grams(traindf['clean'], top_k=20, n=1)\n",
    "top_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_frequencies(top_df):\n",
    "    \"\"\"\n",
    "    Function that receives a dataframe from the \"get_top_n_grams\" function\n",
    "    and plots the frequencies in a bar plot.\n",
    "    \"\"\"\n",
    "    x_labels = top_df[\"Ngram\"][:30]\n",
    "    y_pos = np.arange(len(x_labels))\n",
    "    values = top_df[\"Freq\"][:30]\n",
    "    plt.bar(y_pos, values, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, x_labels)\n",
    "    plt.ylabel('Frequencies')\n",
    "    plt.title('Words')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "plot_frequencies(top_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
