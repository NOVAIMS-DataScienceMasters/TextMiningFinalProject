{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining Final Project 2019 - 2020 - LSTM\n",
    "\n",
    "## Identifying Authors by Their Writings \n",
    "\n",
    "## Authors: \n",
    "- Lara Neves (m20190867) \n",
    "- Susana Paço (m20190821)\n",
    "- Inês Diogo (m20190301)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** To identify the authors of portuguese texts by training a model with labeled texts from the same authors. \n",
    "\n",
    "\n",
    "\n",
    "**First Visual Analysis**\n",
    "* Metadata was detected on the top of most txt's which can turn the bias up in the models. It was agreed that a good idea was to remove this crucial metadata from the corpora and test if it made a significant difference in the final result:\n",
    "    + The name of the authors was detected in most of the metadata;\n",
    "    + References to the authors works was also detected.\n",
    "* Texts from different eras of portuguese \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installs - please uncomment those packages that you don't have within your system in order to install them\n",
    "\n",
    "import sys\n",
    "#!{sys.executable} -m pip install -U unidecode\n",
    "#!{sys.executable} -m pip install -U keras\n",
    "#!{sys.executable} -m pip install -U tensorflow\n",
    "#!{sys.executable} -m pip install -U nltk\n",
    "#!{sys.executable} -m pip install git+https://github.com/textpipe/textpipe.git\n",
    "#!{sys.executable} -m pip install -U spacy\n",
    "#!{sys.executable} -m  spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1. Data Pre-Processing](#DPP)\n",
    "    \n",
    "    * [1.1. Renaming txt Files](#rename)\n",
    "\n",
    "    * [1.2. Extracting Data](#extract)\n",
    "    \n",
    "    * [1.3. Clearing MetaData](#ClearMD)\n",
    "    \n",
    "* [2. Creating a Baseline](#Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"DPP\">\n",
    "\n",
    "## 1. Data Preprocessing\n",
    "\n",
    "</a>\n",
    "\n",
    "\n",
    "<a class=\"anchor\" id=\"rename\">\n",
    "\n",
    "### 1.1. Renaming .txt Files\n",
    "</a>\n",
    "   \n",
    "We will start by renaming the .txt files so there's no duplicates and we create a standardized form to \n",
    "identify each .txt file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the names of the .txt files so there's no duplicates and we create a standardized form to \n",
    "#identify each .txt\n",
    "\n",
    "def change_file_name(author):\n",
    "    i = 0\n",
    "    my_dir_path = \"Data/Corpora/train/\" + author\n",
    "    \n",
    "    for filename in os.listdir(my_dir_path): \n",
    "        \n",
    "        #Define the new and old names with directory path\n",
    "        new_name =str(author) + str(i) + \".txt\"\n",
    "        old_name = my_dir_path + '/' + filename \n",
    "        new_name = my_dir_path + '/' + new_name \n",
    "        \n",
    "        #So it doesn't give out an error when it runs for the second time\n",
    "        # rename all the files \n",
    "        if new_name != old_name: #IT STILL GIVES OUT ERROR\n",
    "            os.rename(old_name, new_name) \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = ['AlmadaNegreiros','CamiloCasteloBranco','EcaDeQueiros','JoseRodriguesSantos','JoseSaramago','LuisaMarquesSilva']\n",
    "authors_sigla = ['AN','CCB','EQ','JRS','JS','LMS']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ONLY RUN ONCE IF THE FILE NAMES ARE THE ORIGINAL otherwise, running a second time, will give an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for a in range(len(authors)):\n",
    "#    change_file_name(authors[a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"extract\">\n",
    "\n",
    "### 1.2. Extracting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a df for one author the respective .txt files in the corpora\n",
    "def create_df_from_txt(author):\n",
    "    my_dir_path = \"Data/Corpora/train/\" + author\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    for file in Path(my_dir_path).iterdir():\n",
    "        with open(file, \"r\",encoding = 'utf8') as file_open:\n",
    "            results[\"id\"].append(file.name)\n",
    "            results[\"text\"].append(file_open.read())\n",
    "            results[\"author\"] = author\n",
    "            file_open.close()\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join together the dataframes from all the authors\n",
    "def join_df(authors):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for a in range(len(authors)):\n",
    "        df = df.append(create_df_from_txt(authors[a]))\n",
    "    df.reset_index(inplace = True, drop = True)    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AlmadaNegreiros3.txt</td>\n",
       "      <td>\\nTitle: Litoral\\n       A Amadeo de Souza Car...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AlmadaNegreiros2.txt</td>\n",
       "      <td>\\n\\nTitle: A Invenção do Dia Claro\\n\\nAuthor: ...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AlmadaNegreiros0.txt</td>\n",
       "      <td>Title: A Scena do Odio\\n\\nAuthor: José de Alma...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AlmadaNegreiros1.txt</td>\n",
       "      <td>Title: O Jardim da Pierrette\\n\\nAuthor: José d...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AlmadaNegreiros5.txt</td>\n",
       "      <td>\\n\\n*JOSÉ DE ALMADA-NEGREIROS*\\n\\n\\n*K4\\n\\no q...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>LuisaMarquesSilva3.txt</td>\n",
       "      <td>CONTROL Z\\nChegou a hora de vos contar. Chegou...</td>\n",
       "      <td>LMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>LuisaMarquesSilva2.txt</td>\n",
       "      <td>O terrível caso do botão assassino\\nLuísa Marq...</td>\n",
       "      <td>LMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>LuisaMarquesSilva0.txt</td>\n",
       "      <td>A BELA HISTÓRIA DE DINIS E BEATRIZ OU REQUIEM ...</td>\n",
       "      <td>LMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>LuisaMarquesSilva1.txt</td>\n",
       "      <td>\\n\\n\\nAcabou-se!\\nLuísa Marques da Silva\\n\\nTí...</td>\n",
       "      <td>LMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>LuisaMarquesSilva8.txt</td>\n",
       "      <td>Título\\nA última história\\n\\nAutora (próximo N...</td>\n",
       "      <td>LMS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                                               text  \\\n",
       "0     AlmadaNegreiros3.txt  \\nTitle: Litoral\\n       A Amadeo de Souza Car...   \n",
       "1     AlmadaNegreiros2.txt  \\n\\nTitle: A Invenção do Dia Claro\\n\\nAuthor: ...   \n",
       "2     AlmadaNegreiros0.txt  Title: A Scena do Odio\\n\\nAuthor: José de Alma...   \n",
       "3     AlmadaNegreiros1.txt  Title: O Jardim da Pierrette\\n\\nAuthor: José d...   \n",
       "4     AlmadaNegreiros5.txt  \\n\\n*JOSÉ DE ALMADA-NEGREIROS*\\n\\n\\n*K4\\n\\no q...   \n",
       "..                     ...                                                ...   \n",
       "58  LuisaMarquesSilva3.txt  CONTROL Z\\nChegou a hora de vos contar. Chegou...   \n",
       "59  LuisaMarquesSilva2.txt  O terrível caso do botão assassino\\nLuísa Marq...   \n",
       "60  LuisaMarquesSilva0.txt  A BELA HISTÓRIA DE DINIS E BEATRIZ OU REQUIEM ...   \n",
       "61  LuisaMarquesSilva1.txt  \\n\\n\\nAcabou-se!\\nLuísa Marques da Silva\\n\\nTí...   \n",
       "62  LuisaMarquesSilva8.txt  Título\\nA última história\\n\\nAutora (próximo N...   \n",
       "\n",
       "   author  \n",
       "0      AN  \n",
       "1      AN  \n",
       "2      AN  \n",
       "3      AN  \n",
       "4      AN  \n",
       "..    ...  \n",
       "58    LMS  \n",
       "59    LMS  \n",
       "60    LMS  \n",
       "61    LMS  \n",
       "62    LMS  \n",
       "\n",
       "[63 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Running all the functions\n",
    "\n",
    "#Creating the training data frame\n",
    "traindf = join_df(authors)\n",
    "\n",
    "#Replacing the name of the authors with labels of their initials\n",
    "for i in range(0,len(authors)):\n",
    "    traindf.author = traindf.author.replace(authors[i],authors_sigla[i])\n",
    "traindf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a df for one author the respective .txt files in the corpora\n",
    "def create_df_from_txttest(numberofwords):\n",
    "    my_dir_path = \"Data/Corpora/test/\"+ str(numberofwords)\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    for file in Path(my_dir_path).iterdir():\n",
    "        with open(file, \"r\",encoding = 'utf8') as file_open:\n",
    "            results[\"id\"].append(file.name)\n",
    "            results[\"text\"].append(file_open.read())\n",
    "            results[\"numberofwords\"] = numberofwords\n",
    "            file_open.close()\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_dftest(numberofwords):\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for a in range(len(numberofwords)):\n",
    "        df = df.append(create_df_from_txttest(numberofwords[a]))\n",
    "    df.reset_index(inplace = True, drop = True)    \n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberofwords = [1000, 500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Running all the functions\n",
    "\n",
    "#Creating the training data frame\n",
    "testdf = join_dftest(numberofwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>numberofwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text6.txt</td>\n",
       "      <td>\"O Senhor ensina pela pena o que o homem não s...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text5.txt</td>\n",
       "      <td>O cahos de cima a descer, a descer com a morta...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text4.txt</td>\n",
       "      <td>Agora, porém, era sem fervor, arrastadamente, ...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text1.txt</td>\n",
       "      <td>Depois, pouco a pouco, a tranquilidade regress...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text3.txt</td>\n",
       "      <td>Quase um mês depois, a época de exames aproxim...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>text2.txt</td>\n",
       "      <td>Justamente como se eu tivesse tido a ideia de ...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>text6.txt</td>\n",
       "      <td>\"O Senhor ensina pela pena o que o homem não s...</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>text5.txt</td>\n",
       "      <td>O cahos de cima a descer, a descer com a morta...</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>text4.txt</td>\n",
       "      <td>Agora, porém, era sem fervor, arrastadamente, ...</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>text1.txt</td>\n",
       "      <td>Depois, pouco a pouco, a tranquilidade regress...</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>text3.txt</td>\n",
       "      <td>Quase um mês depois, a época de exames aproxim...</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>text2.txt</td>\n",
       "      <td>Justamente como se eu tivesse tido a ideia de ...</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text  \\\n",
       "0   text6.txt  \"O Senhor ensina pela pena o que o homem não s...   \n",
       "1   text5.txt  O cahos de cima a descer, a descer com a morta...   \n",
       "2   text4.txt  Agora, porém, era sem fervor, arrastadamente, ...   \n",
       "3   text1.txt  Depois, pouco a pouco, a tranquilidade regress...   \n",
       "4   text3.txt  Quase um mês depois, a época de exames aproxim...   \n",
       "5   text2.txt  Justamente como se eu tivesse tido a ideia de ...   \n",
       "6   text6.txt  \"O Senhor ensina pela pena o que o homem não s...   \n",
       "7   text5.txt  O cahos de cima a descer, a descer com a morta...   \n",
       "8   text4.txt  Agora, porém, era sem fervor, arrastadamente, ...   \n",
       "9   text1.txt  Depois, pouco a pouco, a tranquilidade regress...   \n",
       "10  text3.txt  Quase um mês depois, a época de exames aproxim...   \n",
       "11  text2.txt  Justamente como se eu tivesse tido a ideia de ...   \n",
       "\n",
       "    numberofwords  \n",
       "0            1000  \n",
       "1            1000  \n",
       "2            1000  \n",
       "3            1000  \n",
       "4            1000  \n",
       "5            1000  \n",
       "6             500  \n",
       "7             500  \n",
       "8             500  \n",
       "9             500  \n",
       "10            500  \n",
       "11            500  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From analysis of the texts, these are the supposed y_test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill a y_test column with possible answers\n",
    "\n",
    "\n",
    "\n",
    "testdf.loc[testdf['id'] == 'text1.txt', 'possibleanswer'] = 'JS'\n",
    "testdf.loc[testdf['id'] == 'text2.txt', 'possibleanswer'] = 'AN'\n",
    "testdf.loc[testdf['id'] == 'text3.txt', 'possibleanswer'] = 'unknown'\n",
    "testdf.loc[testdf['id'] == 'text4.txt', 'possibleanswer'] = 'EQ'\n",
    "testdf.loc[testdf['id'] == 'text5.txt', 'possibleanswer'] = 'CCB'\n",
    "testdf.loc[testdf['id'] == 'text6.txt', 'possibleanswer'] = 'JRS'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"clearMD\">\n",
    "\n",
    "### 1.2. Clearing MetaData\n",
    "</a>\n",
    "\n",
    "The majority of the .txt files have metadata at the beginning. This is unnecessary and may introduce noise in our model, as such it may be a good idea to remove it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#author names to remove them from metadata\n",
    "authors = [\"José de Almada Negreiros\", \"José de ALMADA-NEGREIROS\", \"JOSÉ DE ALMADA-NEGREIROS\", \"Almada Negreiros\", \"Camilo Castelo Branco\", \"CAMILLO CASTELLO BRANCO\", \"CAMILLO CASTELLO-BRANCO\", \"Camillo Castello Branco\", \"Eça de Queirós\", \"Eca de Queiros\", \"José Rodrigues dos Santos\",\"Jose Rodrigues dos Santos\", \"JOSÉ RODRIGUES DOS SANTOS\", \"José Saramago\", \"Jose Saramago\", \"JoSÉ SaRamago\", \"Luísa Marques Silva\", \"Luisa Marques Silva\", \"Luísa Marques da Silva\"]  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eça de Queirós</th>\n",
       "      <th>Camilo Castelo Branco</th>\n",
       "      <th>Almada Negreiros</th>\n",
       "      <th>Saramago</th>\n",
       "      <th>José Rodrigues dos Santos</th>\n",
       "      <th>Luísa Marques Silva</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O Mistério da Estrada de Sintra</td>\n",
       "      <td>Anátema</td>\n",
       "      <td>O Moinho</td>\n",
       "      <td>Terra do Pecado</td>\n",
       "      <td>Comunicação, Difusão Cultural, 1992; Prefácio</td>\n",
       "      <td>Acabou-se!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O Crime do Padre Amaro</td>\n",
       "      <td>Os Mistérios de Lisboa</td>\n",
       "      <td>Os Outros</td>\n",
       "      <td>Manual de Pintura e Caligrafia</td>\n",
       "      <td>Crónicas de Guerra I - Da Crimeia a Dachau</td>\n",
       "      <td>Sete Histórias por Acontecer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Tragédia da Rua das Flores</td>\n",
       "      <td>A Filha do Arcediago</td>\n",
       "      <td>23, 2º Andar</td>\n",
       "      <td>Levantado do Chão</td>\n",
       "      <td>Crónicas de Guerra II - De Saigão a Bagdade</td>\n",
       "      <td>e-Medo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Eça de Queirós   Camilo Castelo Branco Almada Negreiros  \\\n",
       "0  O Mistério da Estrada de Sintra                 Anátema         O Moinho   \n",
       "1           O Crime do Padre Amaro  Os Mistérios de Lisboa        Os Outros   \n",
       "2     A Tragédia da Rua das Flores    A Filha do Arcediago    23, 2º Andar    \n",
       "\n",
       "                         Saramago  \\\n",
       "0                 Terra do Pecado   \n",
       "1  Manual de Pintura e Caligrafia   \n",
       "2               Levantado do Chão   \n",
       "\n",
       "                       José Rodrigues dos Santos           Luísa Marques Silva  \n",
       "0  Comunicação, Difusão Cultural, 1992; Prefácio                    Acabou-se!  \n",
       "1     Crónicas de Guerra I - Da Crimeia a Dachau  Sete Histórias por Acontecer  \n",
       "2    Crónicas de Guerra II - De Saigão a Bagdade                        e-Medo  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#book names to stop words\n",
    "authorsandbooks = pd.read_excel('Data/AuthorsAndBooks.xlsx')\n",
    "authorsandbooks.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#authors work to arrays\n",
    "\n",
    "# Eça Queirós books\n",
    "Eca = authorsandbooks['Eça de Queirós']\n",
    "Eca = Eca.dropna()\n",
    "Eca = pd.array(Eca)\n",
    "\n",
    "\n",
    "#Camilo Castelo Branco books\n",
    "camilo = authorsandbooks['Camilo Castelo Branco']\n",
    "camilo = camilo.dropna()\n",
    "camilo = pd.array(camilo)\n",
    "\n",
    "\n",
    "# Almada Negreiros books\n",
    "Almada = authorsandbooks['Almada Negreiros']\n",
    "Almada = Almada.dropna()\n",
    "Almada = pd.array(Almada)\n",
    "\n",
    "\n",
    "# Saramago books\n",
    "Saramago = authorsandbooks['Saramago']\n",
    "Saramago = Saramago.dropna()\n",
    "Saramago = pd.array(Saramago)\n",
    "\n",
    "\n",
    "# José Rodrigues dos Santos books\n",
    "JRodriguesSantos = authorsandbooks['José Rodrigues dos Santos']\n",
    "JRodriguesSantos = JRodriguesSantos.dropna()\n",
    "JRodriguesSantos = pd.array(JRodriguesSantos)\n",
    "\n",
    "\n",
    "# Luísa Marques Silva books\n",
    "luisaMarquesSilva = authorsandbooks['Luísa Marques Silva']\n",
    "luisaMarquesSilva = luisaMarquesSilva.dropna()\n",
    "luisaMarquesSilva = pd.array(luisaMarquesSilva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm #progressbar\n",
    "\n",
    "def removemetadata(doc):\n",
    "    processed_corpus = []\n",
    "    for i in tqdm(range(len(doc))):\n",
    "        text = doc['text'].iloc[i,]\n",
    "        for w in authors:\n",
    "            text = re.sub(w,\"\",text)\n",
    "        for x in Eca:\n",
    "            text = re.sub(x,\"\",text)\n",
    "        for t in camilo:\n",
    "            text = re.sub(t,\"\",text)\n",
    "        for s in Almada:\n",
    "            text = re.sub(s,\"\",text)\n",
    "        for y in Saramago:\n",
    "            text = re.sub(y,\"\",text)\n",
    "        for n in JRodriguesSantos:\n",
    "            text = re.sub(n,\"\",text)\n",
    "        for m in luisaMarquesSilva:\n",
    "            text = re.sub(m,\"\",text)\n",
    "        processed_corpus.append(text)\n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1d7e50c7da4bc192151ab286181c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=63.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18f186cd2684b699acd5135648e749f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create a column to test the results of removing crucial metadata text\n",
    "traindf['removeMetadata'] = removemetadata(traindf)\n",
    "testdf['removeMetadata'] = removemetadata(testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>removeMetadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AlmadaNegreiros3.txt</td>\n",
       "      <td>\\nTitle: Litoral\\n       A Amadeo de Souza Car...</td>\n",
       "      <td>AN</td>\n",
       "      <td>\\nTitle: \\n       \\n\\nAuthor: \\n\\nContributor:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AlmadaNegreiros2.txt</td>\n",
       "      <td>\\n\\nTitle: A Invenção do Dia Claro\\n\\nAuthor: ...</td>\n",
       "      <td>AN</td>\n",
       "      <td>\\n\\nTitle: \\n\\nAuthor: \\n\\nRelease Date: Septe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AlmadaNegreiros0.txt</td>\n",
       "      <td>Title: A Scena do Odio\\n\\nAuthor: José de Alma...</td>\n",
       "      <td>AN</td>\n",
       "      <td>Title: \\n\\nAuthor: \\n\\nRelease Date: September...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AlmadaNegreiros1.txt</td>\n",
       "      <td>Title: O Jardim da Pierrette\\n\\nAuthor: José d...</td>\n",
       "      <td>AN</td>\n",
       "      <td>Title: \\n\\nAuthor: \\n\\nRelease Date: September...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AlmadaNegreiros5.txt</td>\n",
       "      <td>\\n\\n*JOSÉ DE ALMADA-NEGREIROS*\\n\\n\\n*K4\\n\\no q...</td>\n",
       "      <td>AN</td>\n",
       "      <td>\\n\\n**\\n\\n\\n*K4\\n\\no quadrado\\n\\nAZUL*\\n\\nACAB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text  \\\n",
       "0  AlmadaNegreiros3.txt  \\nTitle: Litoral\\n       A Amadeo de Souza Car...   \n",
       "1  AlmadaNegreiros2.txt  \\n\\nTitle: A Invenção do Dia Claro\\n\\nAuthor: ...   \n",
       "2  AlmadaNegreiros0.txt  Title: A Scena do Odio\\n\\nAuthor: José de Alma...   \n",
       "3  AlmadaNegreiros1.txt  Title: O Jardim da Pierrette\\n\\nAuthor: José d...   \n",
       "4  AlmadaNegreiros5.txt  \\n\\n*JOSÉ DE ALMADA-NEGREIROS*\\n\\n\\n*K4\\n\\no q...   \n",
       "\n",
       "  author                                     removeMetadata  \n",
       "0     AN  \\nTitle: \\n       \\n\\nAuthor: \\n\\nContributor:...  \n",
       "1     AN  \\n\\nTitle: \\n\\nAuthor: \\n\\nRelease Date: Septe...  \n",
       "2     AN  Title: \\n\\nAuthor: \\n\\nRelease Date: September...  \n",
       "3     AN  Title: \\n\\nAuthor: \\n\\nRelease Date: September...  \n",
       "4     AN  \\n\\n**\\n\\n\\n*K4\\n\\no quadrado\\n\\nAZUL*\\n\\nACAB...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"clearTexts\">\n",
    "\n",
    "### 1.4. Cleaning Texts\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from tqdm import tqdm_notebook as tqdm #progressbar\n",
    "from unidecode import unidecode\n",
    "from nltk.stem import RSLPStemmer\n",
    "#nltk.download('rslp')\n",
    "\n",
    "#spacy tools\n",
    "import pt_core_news_sm\n",
    "import spacy\n",
    "spacy_nlp = spacy.load('pt_core_news_sm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary functions for preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercasing(text):       \n",
    "    text = text.lower()     \n",
    "    return text\n",
    "\n",
    "def to_string(text):\n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "def lemmatization(word):\n",
    "    lem = WordNetLemmatizer()\n",
    "    word =lem.lemmatize(word)\n",
    "    return word\n",
    "\n",
    "def punctuation(word):\n",
    "    word = re.sub('[\\“\\”\\ \"\\-\\'`~!@#$%^&*()_|+=?;:,.<>\\{\\}\\[\\]\\\\\\/]','', word)\n",
    "    return word\n",
    "\n",
    "def stopwords_nltk(word):\n",
    "    stop_words = set(stopwords.words(\"portuguese\")) \n",
    "    return word in stop_words\n",
    "\n",
    "def stopwords_spacy(word):\n",
    "    return spacy_nlp.vocab[word].is_stop\n",
    "\n",
    "def accents(word):\n",
    "    word = unidecode(word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing 1\n",
    "##### with punctuation and no lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_with_punc(doc, column):\n",
    "    processed_corpus = []\n",
    "    for i in tqdm(range(len(doc))):\n",
    "        text = doc[column].iloc[i,]    \n",
    "    \n",
    "        text = lowercasing(text)\n",
    "        \n",
    "        text = to_string(text)\n",
    "\n",
    "        textfinal = []\n",
    "        for word in text:\n",
    "            if stopwords_nltk(punctuation(word)) or stopwords_spacy(punctuation(word)):\n",
    "                word = re.sub('[^\\“\\”\\ \"\\-\\'`~!@#$%^&*()_|+=?;:,.<>\\{\\}\\[\\]\\\\\\/]','', word)\n",
    "            else:\n",
    "                word\n",
    "            textfinal.append(word)\n",
    "\n",
    "        text = \" \".join(textfinal)\n",
    "    \n",
    "        processed_corpus.append(text)\n",
    "         \n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130c236239e44e5284699310dc67f858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=63.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b170ec406e104cacb6f039a7d73cb75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#this process takes some time, please have patience\n",
    "traindf['clean_with_punc'] = preprocessing_with_punc(traindf, 'removeMetadata')\n",
    "testdf['clean_with_punc'] = preprocessing_with_punc(testdf, 'removeMetadata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing 2 \n",
    "##### no punctuation and no lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_no_punc(doc, column):\n",
    "    processed_corpus = []\n",
    "    for i in tqdm(range(len(doc))):\n",
    "        text = doc[column].iloc[i,]    \n",
    "    \n",
    "        text = lowercasing(text)\n",
    "        \n",
    "        text = to_string(text)\n",
    "        \n",
    "        text = [punctuation(word) for word in text]\n",
    "\n",
    "        text = [word for word in text if (not stopwords_nltk(word)) and (not stopwords_spacy(word))]\n",
    "        \n",
    "        text = [accents(word) for word in text] \n",
    "\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "        processed_corpus.append(text)\n",
    "         \n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26eff206a2f74eac91072aba43a42348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=63.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c766f42dd49745bfaca5fd82405c05d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "traindf['clean_no_punc'] = preprocessing_no_punc(traindf, 'removeMetadata')\n",
    "testdf['clean_no_punc'] = preprocessing_no_punc(testdf, 'removeMetadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing the text into chunks of 500 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_doc(doc,column,n):\n",
    "    newdf = pd.DataFrame()\n",
    "    newdf = newdf.reindex(columns = ['id','chunks','author']) \n",
    "    for i in tqdm(range(len(doc))):\n",
    "        text = doc[column].iloc[i,]\n",
    "\n",
    "        text = text.split()\n",
    "\n",
    "        chunks = [' '.join(text[j:j+n]) for j in range(0,len(text),n)]\n",
    "\n",
    "        for c in chunks:\n",
    "            data = []\n",
    "            values = [doc['id'].iloc[i,], c, doc['author'].iloc[i,]]\n",
    "            a_dictionary = dict(zip(newdf.columns.tolist(), values))\n",
    "            data.append(a_dictionary)\n",
    "            newdf = newdf.append(data)\n",
    "            \n",
    "    newdf.index =[j for j in range(len(newdf))]  \n",
    "              \n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7383a05157a84a1095683b55c6693042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=63.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "traindf_chunks = split_doc(traindf, 'clean_no_punc' ,500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>chunks</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AlmadaNegreiros3.txt</td>\n",
       "      <td>title author contributor amadeu sousa cardoso ...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AlmadaNegreiros2.txt</td>\n",
       "      <td>title author release date september 29 2007 eb...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AlmadaNegreiros2.txt</td>\n",
       "      <td>explicam senhora homem senhoras menina loira h...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AlmadaNegreiros2.txt</td>\n",
       "      <td>cances sorte &lt;&lt;nao tenhas medo estares cabeca ...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AlmadaNegreiros2.txt</td>\n",
       "      <td>atrapalhado pensei responder verdade disse ver...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AlmadaNegreiros2.txt</td>\n",
       "      <td>espera penso verdade for matome gosto bois bar...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AlmadaNegreiros2.txt</td>\n",
       "      <td>sahi casa mae irmao extrangeiro infelicidade m...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AlmadaNegreiros0.txt</td>\n",
       "      <td>title author release date september 16 2007 eb...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AlmadaNegreiros0.txt</td>\n",
       "      <td>sensibilidade manchada vinho lyrio bravo flore...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AlmadaNegreiros0.txt</td>\n",
       "      <td>negro pendao piratas heide corvo marinho beber...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                             chunks  \\\n",
       "0  AlmadaNegreiros3.txt  title author contributor amadeu sousa cardoso ...   \n",
       "1  AlmadaNegreiros2.txt  title author release date september 29 2007 eb...   \n",
       "2  AlmadaNegreiros2.txt  explicam senhora homem senhoras menina loira h...   \n",
       "3  AlmadaNegreiros2.txt  cances sorte <<nao tenhas medo estares cabeca ...   \n",
       "4  AlmadaNegreiros2.txt  atrapalhado pensei responder verdade disse ver...   \n",
       "5  AlmadaNegreiros2.txt  espera penso verdade for matome gosto bois bar...   \n",
       "6  AlmadaNegreiros2.txt  sahi casa mae irmao extrangeiro infelicidade m...   \n",
       "7  AlmadaNegreiros0.txt  title author release date september 16 2007 eb...   \n",
       "8  AlmadaNegreiros0.txt  sensibilidade manchada vinho lyrio bravo flore...   \n",
       "9  AlmadaNegreiros0.txt  negro pendao piratas heide corvo marinho beber...   \n",
       "\n",
       "  author  \n",
       "0     AN  \n",
       "1     AN  \n",
       "2     AN  \n",
       "3     AN  \n",
       "4     AN  \n",
       "5     AN  \n",
       "6     AN  \n",
       "7     AN  \n",
       "8     AN  \n",
       "9     AN  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf_chunks.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import np_utils\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/susanapaco/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>numberofwords</th>\n",
       "      <th>possibleanswer</th>\n",
       "      <th>removeMetadata</th>\n",
       "      <th>clean_with_punc</th>\n",
       "      <th>clean_no_punc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text6.txt</td>\n",
       "      <td>\"O Senhor ensina pela pena o que o homem não s...</td>\n",
       "      <td>1000</td>\n",
       "      <td>JRS</td>\n",
       "      <td>\"O Senhor ensina pela pena o que o homem não s...</td>\n",
       "      <td>\" senhor ensina  pena    homem  .  , alá fala ...</td>\n",
       "      <td>senhor ensina pena homem ala fala directamente...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text  \\\n",
       "0  text6.txt  \"O Senhor ensina pela pena o que o homem não s...   \n",
       "\n",
       "   numberofwords possibleanswer  \\\n",
       "0           1000            JRS   \n",
       "\n",
       "                                      removeMetadata  \\\n",
       "0  \"O Senhor ensina pela pena o que o homem não s...   \n",
       "\n",
       "                                     clean_with_punc  \\\n",
       "0  \" senhor ensina  pena    homem  .  , alá fala ...   \n",
       "\n",
       "                                       clean_no_punc  \n",
       "0  senhor ensina pena homem ala fala directamente...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>removeMetadata</th>\n",
       "      <th>clean_with_punc</th>\n",
       "      <th>clean_no_punc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AlmadaNegreiros3.txt</td>\n",
       "      <td>\\nTitle: Litoral\\n       A Amadeo de Souza Car...</td>\n",
       "      <td>AN</td>\n",
       "      <td>\\nTitle: \\n       \\n\\nAuthor: \\n\\nContributor:...</td>\n",
       "      <td>title: author: contributor: amadeu  sousa card...</td>\n",
       "      <td>title author contributor amadeu sousa cardoso ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text  \\\n",
       "0  AlmadaNegreiros3.txt  \\nTitle: Litoral\\n       A Amadeo de Souza Car...   \n",
       "\n",
       "  author                                     removeMetadata  \\\n",
       "0     AN  \\nTitle: \\n       \\n\\nAuthor: \\n\\nContributor:...   \n",
       "\n",
       "                                     clean_with_punc  \\\n",
       "0  title: author: contributor: amadeu  sousa card...   \n",
       "\n",
       "                                       clean_no_punc  \n",
       "0  title author contributor amadeu sousa cardoso ...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>chunks</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AlmadaNegreiros3.txt</td>\n",
       "      <td>title author contributor amadeu sousa cardoso ...</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                             chunks  \\\n",
       "0  AlmadaNegreiros3.txt  title author contributor amadeu sousa cardoso ...   \n",
       "\n",
       "  author  \n",
       "0     AN  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf_chunks.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing in progress...\n",
      "Pre-processing Train set...\n",
      "Pre-processing Test set...\n",
      "Making some more pre-processing to train and test sets...\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 30, 64)            135296    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 147,910\n",
      "Trainable params: 147,910\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      " - 3s - loss: 1.4929 - accuracy: 0.3175\n",
      "Epoch 2/15\n",
      " - 2s - loss: 1.3041 - accuracy: 0.4169\n",
      "Epoch 3/15\n",
      " - 2s - loss: 1.0197 - accuracy: 0.5488\n",
      "Epoch 4/15\n",
      " - 2s - loss: 0.9381 - accuracy: 0.5804\n",
      "Epoch 5/15\n",
      " - 2s - loss: 0.8503 - accuracy: 0.6322\n",
      "Epoch 6/15\n",
      " - 2s - loss: 0.7729 - accuracy: 0.6862\n",
      "Epoch 7/15\n",
      " - 2s - loss: 0.6712 - accuracy: 0.7444\n",
      "Epoch 8/15\n",
      " - 2s - loss: 0.6195 - accuracy: 0.7696\n",
      "Epoch 9/15\n",
      " - 2s - loss: 0.5661 - accuracy: 0.7962\n",
      "Epoch 10/15\n",
      " - 2s - loss: 0.5086 - accuracy: 0.8166\n",
      "Epoch 11/15\n",
      " - 2s - loss: 0.4757 - accuracy: 0.8296\n",
      "Epoch 12/15\n",
      " - 2s - loss: 0.4450 - accuracy: 0.8463\n",
      "Epoch 13/15\n",
      " - 2s - loss: 0.4171 - accuracy: 0.8545\n",
      "Epoch 14/15\n",
      " - 2s - loss: 0.3960 - accuracy: 0.8629\n",
      "Epoch 15/15\n",
      " - 2s - loss: 0.3885 - accuracy: 0.8666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Tokenize sentences\n",
    "print(\"Tokenizing in progress...\")\n",
    "#1. Train set\n",
    "text_list_train = list(traindf_chunks['chunks'])\n",
    "#text_list_train_lower = [word.lower() for word in text_list_train]\n",
    "tokenized_text_train = [word_tokenize(i) for i in text_list_train]\n",
    "\n",
    "#2. Test set\n",
    "text_list_test = list(testdf['clean_no_punc'])\n",
    "#text_list_test_lower = [word.lower() for word in text_list_test]\n",
    "tokenized_text_test = [word_tokenize(i) for i in text_list_test]\n",
    "\n",
    "#Create vocabulary from train set only\n",
    "list_of_all_words= list(itertools.chain.from_iterable(tokenized_text_test))\n",
    "vocabulary =sorted(list(set(list_of_all_words)))\n",
    "\n",
    "#Remove stopwords (I found out that it makes no difference but you can try on your own)\n",
    "#vocabulary = [word for word in vocabulary if word not in stopwords.words('english')]\n",
    "\n",
    "#--------------------------Pre-processing train and test sets----------------------------------\n",
    "print('Pre-processing Train set...')\n",
    "tokenized_numbers_train = copy.deepcopy(tokenized_text_train)\n",
    "\n",
    "i=-1\n",
    "for list in tokenized_numbers_train:\n",
    "    i=i+1\n",
    "    j=-1\n",
    "    for number in list:\n",
    "        j = j + 1\n",
    "        if tokenized_numbers_train[i][j] in vocabulary:\n",
    "            tokenized_numbers_train[i][j]= vocabulary.index(number)\n",
    "        else:\n",
    "            tokenized_numbers_train[i][j] = 0\n",
    "\n",
    "tokens_train = pd.DataFrame(tokenized_numbers_train, dtype='int32')\n",
    "tokens_train = tokens_train.fillna(0)\n",
    "tokens_train = tokens_train.astype(int)\n",
    "\n",
    "print('Pre-processing Test set...')\n",
    "tokenized_numbers_test = copy.deepcopy(tokenized_text_test)\n",
    "\n",
    "i=-1\n",
    "for list in tokenized_numbers_test:\n",
    "    i=i+1\n",
    "    j=-1\n",
    "    for number in list:\n",
    "        j = j + 1\n",
    "        if tokenized_numbers_test[i][j] in vocabulary:\n",
    "            tokenized_numbers_test[i][j] = vocabulary.index(number)\n",
    "        else:\n",
    "            tokenized_numbers_test[i][j] = 0\n",
    "\n",
    "tokens_test = pd.DataFrame(tokenized_numbers_test, dtype='int32')\n",
    "tokens_test = tokens_test.fillna(0)\n",
    "tokens_test = tokens_test.astype(int)\n",
    "\n",
    "print('Making some more pre-processing to train and test sets...')\n",
    "\n",
    "#Bring both sets to same shape (Choose how many words to use)\n",
    "max_words_in_sentence=30\n",
    "\n",
    "#Shorten or extend Train set to reach selected length\n",
    "if tokens_train.shape[1]>max_words_in_sentence:\n",
    "    tokens_train = tokens_train.drop(tokens_train.columns[[range(max_words_in_sentence,tokens_train.shape[1])]], axis=1)\n",
    "else:\n",
    "    for col in range(tokens_train.shape[1],max_words_in_sentence):\n",
    "        tokens_train[col]=0\n",
    "\n",
    "#Shorten or extend Test set to reach selected length\n",
    "if tokens_test.shape[1] > max_words_in_sentence:\n",
    "    tokens_test = tokens_test.drop(tokens_test.columns[[range(max_words_in_sentence, tokens_test.shape[1])]],\n",
    "                                     axis=1)\n",
    "else:\n",
    "    for col in range(tokens_test.shape[1], max_words_in_sentence):\n",
    "        tokens_test[col] = 0\n",
    "\n",
    "#------------------------------End of Pre-processing----------------------------------------------------\n",
    "\n",
    "#Define train and Test sets\n",
    "train_x = np.array(tokens_train)\n",
    "train_y = np.array(traindf_chunks['author'])\n",
    "\n",
    "test_x = np.array(tokens_test)\n",
    "\n",
    "encoder1 = LabelEncoder()\n",
    "encoder1.fit(train_y)\n",
    "encoded_train_Y = encoder1.transform(train_y)\n",
    "dummy_train_y = np_utils.to_categorical(encoded_train_Y)\n",
    "dummy_train_y.astype(int)\n",
    "\n",
    "l=len(vocabulary)+1\n",
    "inp=train_x.shape[1]\n",
    "\n",
    "#Build an LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(l, 64,input_length=inp))\n",
    "model.add(LSTM(32, dropout=0.4, recurrent_dropout=0.1))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Fit the model\n",
    "model.fit(train_x, dummy_train_y, epochs=15, batch_size=16, verbose=2)\n",
    "\n",
    "\n",
    "# Predict and write to file\n",
    "results = model.predict(test_x)\n",
    "results = pd.DataFrame(results, columns=['AN','CCB','EQ','JRS','JS','LMS'])\n",
    "results['prediction'] = results[['AN','CCB','EQ','JRS','JS','LMS']].idxmax(axis=1)\n",
    "results ['possibleresult'] = testdf['possibleanswer']\n",
    "results.insert(0, \"id\", id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3298/3298 [==============================] - 0s 108us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.26156838877118666, 0.9193450808525085]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(train_x, dummy_train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>AN</th>\n",
       "      <th>CCB</th>\n",
       "      <th>EQ</th>\n",
       "      <th>JRS</th>\n",
       "      <th>JS</th>\n",
       "      <th>LMS</th>\n",
       "      <th>prediction</th>\n",
       "      <th>possibleresult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;built-in function id&gt;</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.994786</td>\n",
       "      <td>0.004744</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>JRS</td>\n",
       "      <td>JRS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;built-in function id&gt;</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.981430</td>\n",
       "      <td>0.017284</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>CCB</td>\n",
       "      <td>CCB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;built-in function id&gt;</td>\n",
       "      <td>0.009836</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>0.979647</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>EQ</td>\n",
       "      <td>EQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;built-in function id&gt;</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.013805</td>\n",
       "      <td>0.981083</td>\n",
       "      <td>0.003849</td>\n",
       "      <td>JS</td>\n",
       "      <td>JS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;built-in function id&gt;</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.997067</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>JRS</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;built-in function id&gt;</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>0.977588</td>\n",
       "      <td>0.020144</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>CCB</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;built-in function id&gt;</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.994786</td>\n",
       "      <td>0.004744</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>JRS</td>\n",
       "      <td>JRS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;built-in function id&gt;</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.981430</td>\n",
       "      <td>0.017284</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>CCB</td>\n",
       "      <td>CCB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;built-in function id&gt;</td>\n",
       "      <td>0.009836</td>\n",
       "      <td>0.006061</td>\n",
       "      <td>0.979647</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>EQ</td>\n",
       "      <td>EQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;built-in function id&gt;</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.013805</td>\n",
       "      <td>0.981083</td>\n",
       "      <td>0.003849</td>\n",
       "      <td>JS</td>\n",
       "      <td>JS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;built-in function id&gt;</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.997067</td>\n",
       "      <td>0.002351</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>JRS</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;built-in function id&gt;</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>0.977588</td>\n",
       "      <td>0.020144</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>CCB</td>\n",
       "      <td>AN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id        AN       CCB        EQ       JRS        JS  \\\n",
       "0   <built-in function id>  0.000011  0.000202  0.000155  0.994786  0.004744   \n",
       "1   <built-in function id>  0.000666  0.981430  0.017284  0.000201  0.000368   \n",
       "2   <built-in function id>  0.009836  0.006061  0.979647  0.001088  0.002809   \n",
       "3   <built-in function id>  0.000220  0.000460  0.000583  0.013805  0.981083   \n",
       "4   <built-in function id>  0.000012  0.000324  0.000165  0.997067  0.002351   \n",
       "5   <built-in function id>  0.000805  0.977588  0.020144  0.000416  0.000950   \n",
       "6   <built-in function id>  0.000011  0.000202  0.000155  0.994786  0.004744   \n",
       "7   <built-in function id>  0.000666  0.981430  0.017284  0.000201  0.000368   \n",
       "8   <built-in function id>  0.009836  0.006061  0.979647  0.001088  0.002809   \n",
       "9   <built-in function id>  0.000220  0.000460  0.000583  0.013805  0.981083   \n",
       "10  <built-in function id>  0.000012  0.000324  0.000165  0.997067  0.002351   \n",
       "11  <built-in function id>  0.000805  0.977588  0.020144  0.000416  0.000950   \n",
       "\n",
       "         LMS prediction possibleresult  \n",
       "0   0.000101        JRS            JRS  \n",
       "1   0.000051        CCB            CCB  \n",
       "2   0.000560         EQ             EQ  \n",
       "3   0.003849         JS             JS  \n",
       "4   0.000081        JRS        unknown  \n",
       "5   0.000097        CCB             AN  \n",
       "6   0.000101        JRS            JRS  \n",
       "7   0.000051        CCB            CCB  \n",
       "8   0.000560         EQ             EQ  \n",
       "9   0.003849         JS             JS  \n",
       "10  0.000081        JRS        unknown  \n",
       "11  0.000097        CCB             AN  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and Loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      " - 2s - loss: 0.3498 - accuracy: 0.8754\n",
      "Epoch 2/15\n",
      " - 2s - loss: 0.3392 - accuracy: 0.8851\n",
      "Epoch 3/15\n",
      " - 2s - loss: 0.3165 - accuracy: 0.8921\n",
      "Epoch 4/15\n",
      " - 2s - loss: 0.3066 - accuracy: 0.8969\n",
      "Epoch 5/15\n",
      " - 2s - loss: 0.2931 - accuracy: 0.9039\n",
      "Epoch 6/15\n",
      " - 2s - loss: 0.2671 - accuracy: 0.9066\n",
      "Epoch 7/15\n",
      " - 2s - loss: 0.2601 - accuracy: 0.9106\n",
      "Epoch 8/15\n",
      " - 2s - loss: 0.2482 - accuracy: 0.9154\n",
      "Epoch 9/15\n",
      " - 2s - loss: 0.2508 - accuracy: 0.9118\n",
      "Epoch 10/15\n",
      " - 2s - loss: 0.2340 - accuracy: 0.9209\n",
      "Epoch 11/15\n",
      " - 2s - loss: 0.2212 - accuracy: 0.9233\n",
      "Epoch 12/15\n",
      " - 2s - loss: 0.2186 - accuracy: 0.9227\n",
      "Epoch 13/15\n",
      " - 2s - loss: 0.1984 - accuracy: 0.9330\n",
      "Epoch 14/15\n",
      " - 2s - loss: 0.1889 - accuracy: 0.9321\n",
      "Epoch 15/15\n",
      " - 2s - loss: 0.1875 - accuracy: 0.9394\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_x, dummy_train_y, epochs=15, batch_size=16, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(n_epochs, model):\n",
    "    loss = model.history['loss']\n",
    "    accuracy = model.history['accuracy']\n",
    "    epochs = range(1,(n_epochs + 1))\n",
    "    plt.plot(epochs, loss, 'g', label='loss')\n",
    "    plt.plot(epochs, accuracy, 'b', label='accuracy')\n",
    "    plt.title('Loss and Accuracy score')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Score')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metric(15,history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
